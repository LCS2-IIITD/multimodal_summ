Video_Presentation,Abstracts
http://videolectures.net/icaps2010_hoos_design/,"High-performance algorithms can be found at the heart of many software systems; they often provide the key to effectively solving the computationally difficult problems encountered in the application areas in which these systems are deployed. Examples of such problems include planning, scheduling, timetabling, resource allocation, computer-aided design and software verification. Many of these problems are NP-hard and considered computationally intractable; nevertheless, these `intractable' problems arise in practice, and finding good solutions to them in many cases tends to become more difficult as economic constraints tighten. In most (if not all) cases, the key to solving such computationally challenging problems lies in the use of high-performance heuristic algorithms, that is, algorithms that make use of mechanisms whose efficacy can be demonstrated empirically, yet remains inaccessible to the analytical techniques used for proving theoretical complexity results. High-performance heuristic algorithms are typically constructed in an iterative, manual process in which the designer gradually introduces or modifies components or mechanisms whose performance is then tested by empirical evaluation on one or more sets of benchmark problems. During this iterative design process, the algorithm designer has to make many decisions, ranging from choices of the heuristic mechanisms to be used and the details of these mechanisms to lower-level implementation details, such as data structures. Some of these choices take the form of parameters, whose values are guessed or determined based on limited experimentation. This traditional approach for designing high-performance algorithms can and often does lead to satisfactory results. However, it tends to be tedious and labour-intensive; furthermore, the resulting algorithms are often unnecessarily complicated, yet fail to realise the full performance potential present in the space of designs that can be built using the same underlying set of components and mechanisms. As an alternative to the traditional, manual algorithm design process, we advocate an approach that uses fully formalised procedures, implemented in software, to permit a human designer to explore large design spaces more effectively, with the aim of realising algorithms with desirable performance characteristics. Computer-aided algorithm design allows human designers to focus on the creative task of specifying a design space in terms of potentially useful components. This design space is then explored using optimisation and machine learning techniques, in combination with significant amounts of computing power, in order to find algorithms that perform well on given sets or distributions of input instances. Automated parameter tuning, algorithm configuration, algorithm portfolios and per-instance algorithm selection are prominent special cases of computer-aided algorithm design and have recently played a pivotal role in improving the state of the art in solving a broad range of challenging combinatorial problems, ranging from propositional satisfiability (SAT) and mixed integer programming to protein structure prediction, course timetabling and planning problems. In this talk, I will introduce computer-aided algorithm design and discuss its main ingredients: design patterns, which provide ways of structuring potentially large spaces of candidate algorithms, and metaalgorithmic optimisation procedures, which are used for finding good designs within these spaces. After explaining how this algorithm design approach differs from and complements related approaches in program synthesis, genetic programming and so-called hyper-heuristics, I will illustrate its success using examples from our own work in SAT-based software verification, timetabling and mixed integer programming. Furthermore, I will argue why this approach can be expected to be particularly useful and effective for building better solvers for rich and diverse classes of combinatorial problems, such as planning and scheduling. Finally, I will outline out how programming by optimisation -- a design paradigm that emphasises the automated construction of performance-optimised algorithm by means of searching large spaces of alternative designs -- has the potential to transform the design of highperformance algorithm from a craft that is based primarily on experience and intuition into a principled and highly effective engineering effort."
http://videolectures.net/icaps2010_vardi_design/,"One of the most significant developments in the area of design verification over the last decade is the development of algorithmic methods for verifying temporal specification of finite-state designs. A frequent criticism against this approach, however, is that verification is done after significant resources have already been invested in the development of the design. Since designs invariably contains errors, verification simply becomes part of the debugging process. The critics argue that the desired goal is to use the specification in the design development process in order to guarantee the development of correct designs. This is called automated design (or synthesis). In this talk I will review 50 years of research on automated design and show how the automata-theoretic approach can be used to solve it."
http://videolectures.net/icaps2010_nardi_robotics/,"Disaster Response Robotics is a challenging domain, where the need for intelligent robotic agents (as opposed to just robots) is motivated both by technical considerations and in a practical application perspective. In emergency scenarios time is critical. Hence, there is a great demand for tools that improve the effectiveness of operations. Although there are specific actions that can be accomplished by a robot, such as for example bomb disposal, a key goal of disaster response robots is to acquire knowledge about the scenario. In fact, a robot can gather data in places that would be either dangerous or inaccessible to the human operator. This often means that the robot is typically not under the visual control of the operator, and sometimes also not connected by a communication link. Consequently, teleoperation can be difficult, if not impossible, and the need arises for intelligent and autonomous capabilities. Moreover, the use of multiple robots naturally stands as a possible breakthrough, given also that disaster scenarios are typically spatially distributed. New challenges hence come up in terms of autonomy, cooperation and collective behaviors. In the first part of the talk, I briefly overview the state of the art in the field of disaster response robotics, in order to support the above sketched analysis. In the second part of the talk, I present some of the research we developed at Sapienza Univ. of Rome, also in collaboration with the Italian Firemen Department. Specifically, I describe some results in Distributed Situation Assessment, Action Planning and Monitoring, Context-based Design of intelligent robotic agents, Multi-robot Teams for disaster response robotics, and Performance Evaluation Metrics for intelligent robotic agents. Throughout the discussion, I focus on several open challenges that need to be addressed to provide effective solutions for Disaster Response Robotics."
http://videolectures.net/icaps2010_domshlak_landmarks/,"Abstractions and landmarks are two powerful mechanisms for devising admissible heuristics for classical planning. Here we aim at putting them together by integrating landmark information into abstractions, and propose a concrete realization of this direction suitable for structural-pattern abstractions, as well as for other abstraction heuristics. Our empirical evaluation shows that landmark information can substantially improve the quality of abstraction heuristic estimates."
http://videolectures.net/icaps2010_xu_greedysearch/,"Greedy search is commonly used in an attempt to generate solutions quickly at the expense of completeness and optimality. In this work, we consider learning sets of weighted action-selection rules for guiding greedy search with application to automated planning. We make two primary contributions over prior work on learning for greedy search. First, we introduce weighted sets of action-selection rules as a new form of control knowledge for greedy search. Prior work has shown the utility of action-selection rules for greedy search, but has treated the rules as hard constraints, resulting in brittleness. Our weighted rule sets allow multiple rules to vote, helping to improve robustness to noisy rules. Second, we give a new iterative learning algorithm for learning weighted rule sets based on RankBoost, an efficient boosting algorithm for ranking. Each iteration considers the actual performance of the current rule set and directs learning based on the observed search errors. This is in contrast to most prior approaches, which learn control knowledge independently of the search process. Our empirical results have shown significant promise for this approach in a number of domains."
http://videolectures.net/icaps2010_srivastava_cacpl/,"The utility of including loops in plans has been long recognized by the planning community. Loops in a plan help increase both its applicability and the compactness of representation. However, progress in finding such plans has been limited largely due to lack of methods for reasoning about the correctness and safety properties of loops of actions. We present novel algorithms for determining the applicability and progress made by a general class of loops of actions. These methods can be used for directing the search for plans with loops towards greater applicability while guaranteeing termination, as well as in post-processing of computed plans to precisely characterize their applicability. Experimental results demonstrate the efficiency of these algorithms."
http://videolectures.net/icaps2010_witwicki_ibpa/,"Decentralized POMDPs are powerful theoretical models for coordinating agents’ decisions in uncertain environments, but the generally-intractable complexity of optimal joint policy construction presents a signiﬁcant obstacle in applying Dec-POMDPs to problems where many agents face many policy choices. Here, we argue that when most agent choices are independent of other agents’ choices, much of this complexity can be avoided: instead of coordinating full policies, agents need only coordinate policy abstractions that explicitly convey the essential interaction inﬂuences. To this end, we develop a novel framework for inﬂuence-based policy abstraction for weakly-coupled transition-dependent Dec-POMDP problems that subsumes several existing approaches. In addition to formally characterizing the space of transition-dependent inﬂuences, we provide a method for computing optimal and approximately-optimal joint policies. We present an initial empirical analysis, over problems with commonly-studied ﬂavors of transition-dependent inﬂuences, that demonstrates the potential computational beneﬁts of inﬂuence-based abstraction over state-of-the-art optimal policy search methods."
http://videolectures.net/icaps2010_boerkoel_asmstp/,"The Simple Temporal Problem (STP) is a popular representation for solving centralized scheduling and planning problems. When scheduling agents are associated with different users who need to coordinate some of their activities, however, considerations such as privacy and scalability suggest solving the joint STP in a more distributed manner. Building on recent advances in STP algorithms that exploit loosely-coupled problem structure, this paper develops and evaluates algorithms for solving the multiagent STP. We define a partitioning of the multiagent STP with provable privacy guarantees, and show that our algorithms can exploit this partitioning while still finding the tightest consistent bounds on timepoints that must be coordinated across agents. We also demonstrate empirically that our algorithms can exploit concurrent computation, leading to solution time speed-ups over state-of-the-art centralized approaches, and enabling scalability to problems involving larger numbers of loosely-coupled agents."
http://videolectures.net/icaps2010_traverso_fureinternet/,"A lot of work has been done so far in the field of planning for web or software services. In this work, major tasks can be described in a planning framework: modeling of software services leads to a planning domain, their automated composition can be done by plan generation, their monitoring becomes monitoring of plan executions, and adaptation can done by re-planning. Things change radically when we move to the framework envisioned by Future Internet. One of the major promises of the vision of Future Internet is the so called  ŇInternet of ServicesÓ, where applications ŇliveÓ in the network and are available to end users as Ňreal servicesÓ are available today to consumers in everyday life. According to this vision, software services are just software components that provide electronic access to Ňreal servicesÓ (e.g., a software service for travel booking allows us to access the actual service behind it, namely Ňthe possibility of travelingÓ). ŇReal servicesÓ are however very different from the corresponding software services, since they differ in their main characteristics, such as their duration, their accessibility, their constraints and conflicts, and their connection with the real world, which makes them highly dynamic. This new vision requires a shift in the research approach, as well as in the corresponding planning framework: Modeling should describe how the use of real services affects their consumers;  Composition does not consist anymore in the generation of a new composed service, it becomes a task that finds relations among services based on emergent needs, constraints, opportunities of the consumers; Monitoring should not check software executions but rather focus on properties of the physical environment where the real services operate; Adaptation should move from reaction to changes in software services to reaction to changes in real services, in the physical environment where they operate, and to usersŐ behaviors. In this tutorial I will briefly summarize the traditional approaches to planning for software services, and I will then focus on the new research challenges for the future internet of services and how they are related to planning."
http://videolectures.net/icaps2010_sanner_trafficcontrol/,"The ubiquity of urban traffic congestion and the fundamental impact that better traffic control can have on urban environments makes it an important research topic for the automated planning and scheduling community.  To reduce traffic congestion by just 10% can have massive economic, environmental, and social benefits for urban communities.  While a great deal of traffic theory has been developed over the years, the practical techniques utilized in most urban traffic control situations are surprisingly simple and rely on extensive manual tuning via trial and error; in a nutshell, there is a lot of room for improvement for automated traffic control techniques that can deal with the full complexities of traffic management in an online control setting.  The purpose of this tutorial is to describe the theory of traffic simulation (basic modeling including micro- and macro-simulation) and control (single and multi-intersection control from both theoretical and practical perspectives) in order to expose the research topics (extremely large continuous state spaces, highly parallel continuous action spaces with nonlinear effects) that need to be addressed if the planning and scheduling community is to make progress in this challenging, but high-impact application domain."
http://videolectures.net/icaps2010_sanner_lhsp/,"The recent past has seen a resurge of interest in landmarks for heuristic-search planning. Landmarks are subgoals that have to become true at some point during any plan for a given task. They can be used in various ways to assist the search for a plan. This tutorial will give an overview of how landmarks can be identified for a given task and how they may be exploited for planning. In detail, the following topics will be covered:     1. Definitions of landmarks and orderings, including action landmarks.    2. Landmark discovery procedures, including back-chaining from goals, path analysis in domain transition graphs, and forward propagation of information in the planning graph.    3. Methods to exploit landmarks during planning, including as intermediate goals, in the LAMA heuristic, as admissible heuristic, and as problem enrichment via temporal formulas."
http://videolectures.net/icaps2010_fern_mcpbprp/,"Many planning applications are difficult to model in standard domain description languages. However, with out the limitations of a particular language, it is often possible to obtain or construct an exact or approximate simulator of the application domain. Monte-Carlo planning is an area that studies algorithms for sequential decision making when such a simulator is available. In recent years, advances in Monte-Carlo planning have lead to significant advances in applications ranging from computer networking, to real-time strategy games, to computer Go. This tutorial will cover the basic principles and theory underlying Monte-Carlo planning and also the recent advances. Emphasis will be placed on practical approaches with illustrating applications. The tutorial will start from first principles and will not assume prior knowledge of Monte-Carlo techniques."
