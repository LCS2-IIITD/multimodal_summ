Video_Presentation,Abstracts
http://videolectures.net/icml2010_stodden_rric/,"Scientific computation is emerging as absolutely central to the scientific method, but the prevalence of very relaxed practices is leading to a credibility crisis. Reproducible computational research, in which all details of computations—code and data—are made conveniently available to others, is a necessary response to this crisis. Results from a 2009 survey of the Machine Learning community (NIPS participants) designed to elucidate factors that affect data and code sharing will be presented. Intellectual property concerns create a significant barrier to sharing, and I will also present work on the “Reproducible Research Standard” giving open licensing options designed to create an intellectual property framework for scientists consonant with longstanding scientific norms and facilitating reproducible research."
http://videolectures.net/icml2010_holger_ujmp/,"The Universal Java Matrix Package (UJMP) is an open source Java library which provides sparse and dense matrix classes, as well as a large number of calculations for linear algebra like matrix multiplication or matrix inverse. Operations such as mean, correlation, standard deviation, replacement of missing values or the calculation of mutual information are supported also. The Universal Java Matrix Package provides various visualization methods, import and export filters for a large number of file formats, and the possibility to link to tables in JDBC databases. Multi-dimensional matrices as well as generic matrices with a specified object type are supported and very large matrices with up to 2^63 rows and columns can be handled even when they do not fit into memory. A central concept of UJMP is the separation of interfaces, abstract classes and their implementations, which makes it very easy to exchange the underlying data storage. Thus, a matrix in our framework can be an array of values in main memory, a file on disk or a table in an SQL database. In fact, the actual storage implementation becomes secondary and UJMP can even integrate other matrix libraries such as JAMA or Colt, making UJMP’s visualization and import and export filters available to these libraries. On the other hand, UJMP can also decide to redirect calculations to other matrix libraries, depending on matrix size and computer hardware. UJMP uses multiple threads for calculations, which results in much better performance compared to JAMA or Colt on modern hardware. UJMP also includes interfaces to Matlab, Octave and R, which makes it easy to perform calculations not available in Java. While some parts of UJMP are pretty stable by now, a lot of development is still going on in other parts. Developers are welcome to contribute!"
http://videolectures.net/icml2010_sonnenburg_shog/,"The SHOGUN machine learning toolbox's focus is on large scale kernel methods and especially on Support Vector Machines (SVM). It comes with a generic interface for kernel machines and features 15 different SVM implementations that all access features in a unified way via a general kernel framework or in case of linear SVMs so called ""DotFeatures"", i.e., features providing a minimalistic set of operations (like the dot product)."
http://videolectures.net/icml2010_sonnenburg_tnsau/,"Recently, mloss.org has enabled machine learning researchers to register their software and allow other researchers to easily find, download, and reuse software matching their interests. Currently, more than 200 projects are listed. Furthermore, the Journal of Machine Learning Research now accepts papers to its special Open Source Software track, in which papers describing peer-reviewed software can be published, as a further incentive for researchers to publish their software under an open source license. Since its inception, in October 2007, seven papers have been published in this track with more papers currently under review. So far, the initiative has been highly successful, but has focused mostly on the ”method” side of the problem to make machine learning research more reproducible. Hence we see the need to initiate a companion project to mloss.org which focuses on the free exchange and benchmarking of datasets. Additionally, this new repository will emphasise the precise specification of machine learning tasks: detailed definitions of datasets to be used (possibly including feature extraction or other preprocessing steps) together with the desired operation to be performed and the relevant performance metric. Finally, a solution to such a task would provide details of how to apply a general software package (such as on mloss.org) to this particular problem instance, as well as the obtained numerical performance measures. This project will thus focus on providing a platform for publishing, exchanging, collecting, and discussing such data sets, tasks, and solutions for challenging machine learning problems."
http://videolectures.net/icml2010_lowd_libra/,"The Libra machine learning toolkit includes implementations of a variety of algorithms for learning and inference with Bayesian networks and arithmetic circuits: Learning algorithms -- Structure learning for BNs and ACs; Chow-Liu algorithm; AC weight learning Inference algorithms - Mean field, belief propagation, Gibbs sampling, AC variable elimination, AC exact inference Libra's strength is exploiting context-specific independence (such as decision tree CPDs) to allow exact inference in models with high treewidth."
http://videolectures.net/icml2010_jaimovich_fast/,"The FastInf C++ library is designed to perform memory and time efficient approximate inference in large-scale discrete undirected graphical models. The focus of the library is propagation based approximate inference methods, ranging from the basic loopy belief propagation algorithm to propagation based on convex free energies. Various message scheduling schemes that improve on the standard synchronous or asynchronous approaches are included. Also implemented are a clique tree based exact inference, Gibbs sampling, and the mean field algorithm. In addition to inference, FastInf provides parameter estimation capabilities as well as representation and learning of shared parameters. It offers a rich interface that facilitates extension of the basic classes to other inference and learning methods."
http://videolectures.net/icml2010_schaul_pyb/,"PyBrain is a versatile machine learning library for Python. Its goal is to provide flexible, easy-to-use yet still powerful algorithms for machine learning tasks, including a variety of predefined environments and benchmarks to test and compare algorithms. Implemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (multidimensional) recurrent neural networks and evolution strategies like CMA-ES, NES or FEM."
http://videolectures.net/icml2010_gohr_jstacs/,"Sequence analysis is one of the major subjects of bioinformatics. Several existing libraries combine the representation of biological sequences with exact and approximate pattern matching as well as alignment algorithms. We present Jstacs, an open source Java library, which focuses on the statistical analysis of biological sequences instead. Jstacs comprises an efficient representation of sequence data and provides implementations of many statistical models with generative and discriminative approaches for parameter learning. Using Jstacs, classifiers can be assessed and compared on test datasets or by cross-validation experiments evaluating several performance measures. Due to its strictly object-oriented design Jstacs is easy to use and readily extensible."
http://videolectures.net/icml2010_braun_jblas/,"jblas is a fast linear algebra library for Java. jblas is based on BLAS and LAPACK, the de-facto industry standard for matrix computations, and uses state-of-the-art implementations like ATLAS for all its computational routines, making jBLAS very fast. jblas can is essentially a light-wight wrapper around the BLAS and LAPACK routines. These packages have originated in the Fortran community which explains their archaic API. On the other hand modern implementations are hard to beat performance wise. jblas aims to make this functionality available to Java programmers such that they do not have to worry about writing JNI interfaces and calling conventions of Fortran code. jblas is the only actively developed matrix library which is based on native implementations (The other such project is netlib-java which is apparently not maintained anymore). Therefore, jblas is much faster than other projects, in particular for large complex tasks like matrix-matrix multiplication, solving linear equations, or eigenproblems."
http://videolectures.net/icml2010_varaquaux_scik/,Scikits.learn is a Python module integrating classique machine learning algorithmes in the tightly-nit world of scientific Python packages It aims to provide simple and efficient solutions to learning problems that are accessible to everybody and reusable in various contexts: machine-learning as a versatile tool for science and engineering.
http://videolectures.net/icml2010_spyromitros_mumu/,"Mulan is an open-source Java library for learning from multi-label datasets. Multi-label datasets consist of training examples of a target function that has multiple binary target variables. This means that each item of a multi-label dataset can be a member of multiple categories or annotated by many labels (classes). This is actually the nature of many real world problems such as semantic annotation of images and video, web page categorization, direct marketing, functional genomics and music categorization into genres and emotions. An introduction on mining multi-label data is provided in (Tsoumakas et al., 2010). Currently, the library includes a variety of state-of-the-art algorithms for performing the following major multi-label learning tasks: Classification. This task is concerned with outputting a bipartition of the labels into relevant and irrelevant ones for a given input instance. Ranking. This task is concerned with outputting an ordering of the labels, according to their relevance for a given data item Classification and ranking. A combination of the two tasks mentioned-above. In addition, the library offers the following features: Feature selection. Simple baseline methods are currently supported. Evaluation. Classes that calculate a large variety of evaluation measures through hold-out evaluation and cross-validation. As already mentioned, Mulan is a library. As such, it offers only programmatic API to the library users. There is no graphical user interface (GUI) available. The possibility to use the library via command line, is also currently not supported. The Getting Started page in the Documentation section is the ideal place to start exploring Mulan."
http://videolectures.net/icml2010_allauzen_opke/,"The OpenKernel library is an open-source software library for designing, combining, learning and using kernels for machine learning applications The library supports the design and use of kernels defined over dense and sparse real vectors, as well as over sequences or distributions of sequences. For dense and sparse features, the library provides implementation of the classical kernels: linear, polynomial, Gaussian and sigmoid. For sequences and distributions of sequences, the library implements the rational kernel framework of Cortes et al. (JMLR, 2004). The library supplies the following sequence kernels: n -gram kernels, gappy n-gram kernels, mismatch kernels (Leslie et al., 2004), and gives the utilities for creating arbitrary rational kernels simply by providing the weighted finite-state transducers they are based on. Kernels can be combined by taking their sum or their product, and can be composed with a polynomial, a Gaussian or a sigmoid. They support on-demand evaluation and caching. In addition to its own binary format, the library uses the ASCII format of LIBSVM/LIBLINEAR/SVMlight for representing features (and precomputed kernels for LIBSVM). Finally, the OpenKernel library also includes several options for using training data to automatically combine multiple kernels. This is particularly useful when the single best kernel for the task is not known. The algorithms implemented include L1-regularized linear combinations (Lanckriet et al. JMLR 2004); L2-regularized linear combinations (Cortes et al. UAI 2009); L2-regularized quadratic combinations (Cortes et al. NIPS 2009), as well as kernel correlation, or alignment (Cortes et al. ICML 2010), based combinations. Specialized efficient versions of these algorithms are also made available for weighting features and sparseness and can be used to further improve efficiency. The output kernels can be easily used in conjunction with LIBSVM, SVMlight and included kernel ridge regression implementations. Full reference documentation, tutorials and examples (with formatted datasets) are included. The library is an open-source project distributed under the Apache license (2.0). This work has been partially supported by Google Inc. The library uses the OpenFst library for representing and manipulating weighted finite-state transducers."
http://videolectures.net/icml2010_kegl_mubu/,"AdaBoost [Freund-Schapire, 1997] is one of the best off-the-shelf supervised classification methods developed in the last fifteen years. Despite (or perhaps due to?) its simplicity and versatility, it is suprisingly under-represented in the family of open softwares. The goal of this submission is to fill this gap. Our implementation is based on the AdaBoost.MH algorithm [Schapire-Singer, 1999]. It is an intrinsically multi-class classification method (unlike SVM for example), and it was easy to extend to multi-label or multi-task classification (when one item can belong to several classes). The program package can be divided into four modules that can be changed more-or-less independently depending on the application. The strong learner. It tells you how to boost. The main boosting engine is AdaBoost.MH, but we have also implemented FilterBoost for a research project. Other possible strong learners could be LogitBoost and ADTrees. The base (or weak) learner. It tells you what features to boost. Right now we have two basic (feature-wise) base learners: decision stumps for real-valued features and indicators for nominal features. We have two meta base learners: trees and products. They can use any base learner and construct a generic complex base learner using a ""classic"" tree-structure (decision trees), or using the product of simple base learners (self advertisement: boosting products of stumps is the best reported no-domain-knowledge algorithm on MNIST after Hinton and Salakhutdinov's deep belief nets). We have also implemented Haar filters [Viola-Jones, 2004] for image classification, a meta base learner that uses stumps over a high dimensional feature space computed ""on the fly"". It is a nice example of a domain dependent base learner that works hand-in-hand with its appropriate data structure. The data representation. The basic data structure is a matrix of observations with a vector of labels. We also have multi-label classification when the label data is also a full matrix. In addition, we have sparse data representation for both the observation matrix and the label matrix. In general, base learners are implemented to work with their own data representation (for example, sparse stumps work on sparse observation matrices, or Haar filters work on a integral image data representation. Data parser. We can read in data in arff and svmlight formats. The base learner/data structure combinations cover a large spectrum of possible applications, but the main advantage of the package is that it is easy (for the advanced user) to adapt MultiBoost to a specific (non-standard) application by implementing the base learner and data structure interfaces that work together. The source code is available from the website multiboost.org. It can be compiled on Mac OS X, Linux, and Microsoft Windows. The interface is command line execution with switches."
http://videolectures.net/icml2010_ferrer_gidoc/,"Transcription of handwritten text in (old) documents is an important, time-consuming task for digital libraries. It might be carried out by first processing all document images off-line, and then manually supervising system transcriptions to edit incorrect parts. However, current techniques for automatic page layout analysis, text line detection and handwriting recognition are still far from perfect, and thus post-editing system output is not clearly better than simply ignoring it. A more effective approach to transcribe old text documents is to follow an interactive- predictive paradigm in which both, the system is guided by the user, and the user is assisted by the system to complete the transcription task as efficiently as possible. Following this approach, a system prototype called GIDOC (Gimp-based Interactive transcription of old text DOCuments) has been developed to provide user-friendly, integrated support for interactive-predictive layout analysis, line detection and handwriting transcription. GIDOC is designed to work with (large) collections of homogeneous documents, that is, of similar structure and writing styles. They are annotated sequentially, by (par- tially) supervising hypotheses drawn from statistical models that are constantly updated with an increasing number of available annotated documents. And this is done at different annotation levels. For instance, at the level of page layout analysis, GIDOC uses a novel text block detection method in which conventional, memoryless techniques are improved with a “history” model of text block positions. Similarly, at the level of text line image transcription, GIDOC includes a handwriting recognizer which is steadily improved with a growing number of (partially) supervised transcriptions."
http://videolectures.net/icml2010_lahti_dmt/,"Investigation of dependencies between multiple data sources allows the discovery of regularities and interactions that are not seen in individual data sets. The increasing availability of co-occurring measurement data in computational biology, social sciences, and in other domains emphasizes the need for practical implementations of general-purpose dependency modeling algorithms. The project collects various dependency modeling approaches into a unified toolbox. The techniques for the discovery and analysis of statistical dependencies are based on well-established models such as probabilistic canonical correlation analysis and multi-task learning whose applicability has been demonstrated in previous case studies."
http://videolectures.net/icml2010_ben_david_cwsi/,"Consider the task of clustering university web pages based on the graph of links between these pages. Can clusters of ""functionally similar"" pages be detected from just this link structure? Note that this is a clustering task in which one starts without any prior knowledge of any similarity or distance measure between the domain elements. All the information in the input comes as objective, observed, binary relations among the objects. These relations are not similarity links. For example, the cluster of professors pages have very internal links, whereas the cluster of service pages have lots of internal links. What we are looking for are clusters whose members share similar link patterns with respect to the other clusters.  We propose a formal model for such clustering tasks. Our model is based on an objective function that measures the homogeneity of between-clusters links. I shall discuss the computational complexity of finding a clustering with minimal objective cost and describe some hardness results as well as efficient approximation algorithms.  The talk is (partly) based on work with Sharon Wulff."
http://videolectures.net/icml2010_balcan_lwsf/,"Kernel functions have become an extremely popular tool in machine learning, with many applications and an attractive theory. This theory views a kernel as performing an implicit mapping of data points into a possibly very high dimensional space, and describes a kernel function as being good for a given learning problem if data is separable by a large margin in that implicit space. In this talk I will describe an alternative, more general, theory of learning with similarity functions (i.e., sufficient conditions for a similarity function to allow one to learn well) that does not require reference to implicit spaces, and does not require the function to be positive semi-definite (or even symmetric).  In particular, I will describe a notion of a good similarity function for a given learning problem that (a) is fairly natural and intuitive (it does not require an implicit space and allows for functions that are not positive semi-definite), (b) is a sufficient condition for learning well, and (c) strictly generalizes the notion of a large-margin kernel function in that any such kernel is also a good similarity function, though not necessarily vice-versa."
http://videolectures.net/icml2010_smola_fcfm/,"Recent work on collaborative filtering has led to a large number of both scalable and theoretically well founded algorithms. In this paper, we show that collaborative filtering and multitask learning are innately closely connected. In particular, the 'learning the kernel' paradigm in multitask learning turns out to be identical to a Ky-Fan norm minimization. This allows us to “import” collaborative filtering techniques into multitask learning and vice versa; in particular, we solve a multitask learning problem where the tasks also have features. We show the feasibility of our approach on two large real-world multitask learning applications.  Joint work with Markus Weimer, Wei Chu, Deepayan Chakrabarti."
http://videolectures.net/icml2010_shashua_ophm/,"We consider the problem of finding a matching between two sets of features, given complex relations among them, going beyond pairwise. We derive the hyper-graph matching problem in a probabilistic setting represented by a convex optimization. First, we formalize a soft matching criterion that emerges from a probabilistic interpretation of the problem input and output, as opposed to previous methods that treat soft matching as a mere relaxation of the hard matching problem. Second, the model induces an algebraic relation between the hyper-edge weight matrix and the desired vertex-to-vertex probabilistic matching. Third, the model explains some of the graph matching normalization proposed in the past on a heuristic basis such as doubly stochastic normalizations of the edge weights. A key benefit of the model is that the global optimum of the matching criteria can be found via an iterative successive projection algorithm. The algorithm reduces to the well known Sinkhorn row/column matrix normalization procedure in the special case when the two graphs have the same number of vertices and a complete matching is desired. Another benefit of our model is the straightforward scalability from graphs to hyper-graphs.  The work was done with Ron Zass (and made its debut in CVPR 2008)"
http://videolectures.net/icml2010_krauthgamer_amnda/,"Let us define the dimension of a metric space as the minimum k>0 such that every ball in the metric space can be covered by 2^k balls of half the radius. This definition has several attractive features besides being applicable to every metric space. For instance, it coincides with the standard notion of dimension in Euclidean spaces, but captures also nonlinear structures such as manifolds.  Metric spaces of low dimension (under the above definition) occur naturally in many contexts. I will discuss recent theoretical results regarding such metric spaces, including questions such as embeddability, dimension reduction, Nearest Neighbor Search, and large-margin classification, the common thread being that low dimension implies algorithmic efficiency."
http://videolectures.net/icml2010_cesa_bianchi_salg/,"Networked data are found in a variety of domains: Web, social networks, biological networks, and many others. In learning tasks, networked data are often represented as a weighted graph whose edge weights reflect the similarity between incident nodes. In this talk, we consider the problem of classifying the nodes of an arbitrary given graph in the game-theoretic mistake bound model. We characterize the optimal predictive performance in terms of the cutsize of the graph's random spanning tree, and describe a randomized prediction algorithm achieving the optimal performance while running in expected time sublinear in the graph size (on most graphs). These results are then extended to the active learning model, where training labels are obtained by querying nodes selected by the algorithm. We describe a fast query placement strategy that, in the special case of trees, achieves the optimal number of mistakes when classifying the non-queried nodes.  Joint work with: Claudio Gentile, Fabio Vitale and Giovanni Zappella."
http://videolectures.net/icml2010_von_luxburg_ancr/,"Non-geometric data is often represented in form of a graph where edges represent similarity or local relationships between instances. One elegant way to exploit the global structure of the graph is implemented by the commute distance (also known as resistance distance). Supposedly it has the property that vertices from the same cluster are ""close"" to each other whereas vertices from different clusters are ""far"" from each other. We study the behavior of the commute distance as the size of the underlying graph increases. We prove that the commute distance converges to an expression that does not take into account the structure of the graph and that is completely meaningless as a distance function on the graph. Consequently, the use of the raw commute distance for machine learning purposes is strongly discouraged for large graphs and in high dimensions. We suspect that a similar behavior holds for several other distances on graphs."
