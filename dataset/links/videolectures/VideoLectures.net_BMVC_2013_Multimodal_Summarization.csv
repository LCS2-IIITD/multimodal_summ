Video_Presentation,Abstracts
http://videolectures.net/bmvc2013_zisserman_video_search/,"We would like to be able to find anything in an image or video dataset. The talk will describe our progress on visual search for finding people, specific objects and categories in large scale video datasets. The novelty is that the item of interest can be specified at run time by a text query, and a discriminative classifier for that item is then learnt on-the-fly using images downloaded from Google Image search. We will compare state of the art encoding methods for the problem, and discuss the choices in achieving the best trade-off between three important performance measures for a realtime system of this kind, namely: (i) accuracy, (ii) memory footprint, and (iii) speed. We will also describe steps to achieving `total recall'. There will be demonstrations on a large scale video dataset of BBC broadcasts. This is joint work with Relja Arandjelovic, Ken Chatfield and Omkar Parkhi."
http://videolectures.net/bmvc2013_dellaert_factor_graphs/,"Simultaneous Localization and Mapping (SLAM) and Structure from Motion (SFM) are important and closely related problems in robotics and vision. I will show how both SLAM and SFM instances can be posed in terms of a graphical model, a factor graph, and that inference in these graphs can be understood as variable elimination. The overarching theme of the talk will be to emphasize the advantages and intuition that come with seeing these problems in terms of graphical models. For example, common computational tricks, such as the Schur complement trick in SFM, are simple choices about the order in which to eliminate the graph. In addition, while the graphical model perspective is completely general, linearizing the non-linear factors and assuming Gaussian noise yields the familiar direct linear solvers such as Cholesky and QR factorization. Based on these insights, we have developed both batch and incremental algorithms defined on graphs in the SLAM/SFM domain. In addition to direct methods, we recently worked on efficient iterative methods that use subgraphs of these factor graphs as pre-conditioners in a conjugate gradient scheme. Finally, we are now looking into how optimal control can be seamlessly integrated with the estimation algorithms for use in autonomous vehicles."
http://videolectures.net/bmvc2013_coates_machine_vision/,"Machine learning algorithms have freed practitioners from many error-prone, hand-engineered components for making decisions in common machine vision tasks such as object recognition. A major source of difficulty, however, is that such learning systems still rely on many hand-built components like sophisticated feature extractors that attempt to identify higher-level patterns in images that typical learning algorithms cannot discover on their own. ""Deep learning"" and ""representation learning"" algorithms aim to remove this hurdle by learning higher-level representations automatically from data and have led to recent successes in vision, speech, and language tasks. This tutorial will introduce the basic components of deep learning algorithms and practical techniques for debugging and applying these methods to machine vision problems. The first part of the tutorial will cover neural network models and basic training approaches including error back-propagation and numerical optimization methods, with image classification as a motivating application. The second part will cover additional (sometimes domain-specific) techniques to improve the performance of these algorithms and apply them to other vision tasks including detection and image segmentation. With these tools, audience members will understand how deep learning algorithms work and how they are used in practical applications with sufficient knowledge to complete a hands-on tutorial available on the web. We will conclude with a brief high-level overview of other important topics and results in deep learning research."
http://videolectures.net/bmvc2013_krause_machine_learning/,"Numerous problems in machine learning and vision are inherently discrete. More often than not, these lead to challenging optimization problems. While convexity is an important property when solving continuous optimization problems, submodularity, often viewed as a discrete analog of convexity, is key to solving many discrete problems. Its characterizing property, diminishing marginal returns, appears naturally in a multitude of settings. While submodularity has long been recognized in combinatorial optimization and game theory, it has seen a recent surge of interest in theoretical computer science, machine learning and computer vision. This tutorial will introduce the concept of submodularity and its basic properties, and outline recent research directions -- such as new approaches towards large-scale optimization and sequential decision making tasks. We will discuss recent applications to challenging machine learning and vision problems such as high-order graphical model inference, structured sparse modeling, multiple object detection, active sensing etc. The tutorial will not assume any specific prior knowledge on the subject."
http://videolectures.net/bmvc2013_pons_moll_regression/,"We present a new method for inferring dense data to model correspondences, focusing on the application of human pose estimation from depth images. Recent work proposed the use of regression forests to quickly predict correspondences between depth pixels and points on a 3D human mesh model. That work, however, used a proxy forest training objective based on the classification of depth pixels to body parts. In contrast, we introduce Metric Space Information Gain (MSIG), a new decision forest training objective designed to directly optimize the entropy of distributions in a metric space. When applied to a model surface, viewed as a metric space defined by geodesic distances, MSIG aims to minimize image-to-model correspondence uncertainty. A naïve implementation of MSIG would scale quadratically with the number of training examples. As this is intractable for large datasets, we propose a method to compute MSIG in linear time. Our method is a principled generalization of the proxy classification objective, and does not require an extrinsic isometric embedding of the model surface in Euclidean space. Our experiments demonstrate that this leads to correspondences that are considerably more accurate than state of the art, using far fewer training images."
http://videolectures.net/bmvc2013_rodriguez_text_recognition/,"The standard approach to recognizing text in images consists in first classifying local image regions into candidate characters and then combining them with high-level word models such as conditional random fields (CRF). This paper explores a new paradigm that departs from this bottom-up view. We propose to embed word labels and word images into a common Euclidean space. Given a word image to be recognized, the text recognition problem is cast as one of retrieval: find the closest word label in this space. This common space is learned using the Structured SVM (SSVM) framework by enforcing matching label-image pairs to be closer than non-matching pairs. This method presents the following advantages: it does not require costly pre- or post-processing operations, it allows for the recognition of never-seen-before words and the recognition process is efficient. Experiments are performed on two challenging datasets (one of license plates and one of scene text) and show that the proposed method is competitive with standard bottom-up approaches to text recognition."
http://videolectures.net/bmvc2013_shah_change_detection/,"In this paper, we propose a framework that can be used for detecting relevant changes in highly dynamic scenes, where the background has several changing elements. To establish a clear distinction between what is relevant and what is not is a very challenging task. Therefore, we ﬁrst categorize the changes into two main classes called ordinary changes and relevant changes. Detected changes are considered as irrelevant if they are recurrent elements and changes pertaining on the dynamic background of the scene. The proposed framework makes use of a set of orthogonal linear transforms to capture spatiotemporal signatures of local ordinary change patterns and subsequently employ them in the detection of relevant changes. The use of this framework is demonstrated in a variety of videos with highly dynamic backgrounds including lakes, pools, and roads. Compared to existing methods reported on the same test videos, the proposed framework detects the relevant changes more accurately."
http://videolectures.net/bmvc2013_simonyan_vector_faces/,"Several recent papers on automatic face verification have significantly raised the performance bar by developing novel, specialised representations that outperform standard features such as SIFT for this problem. This paper makes two contributions: first, and somewhat surprisingly, we show that Fisher vectors on densely sampled SIFT features, i.e. an off-the-shelf object recognition representation, are capable of achieving state-of-the-art face verification performance on the challenging “Labeled Faces in the Wild” benchmark; second, since Fisher vectors are very high dimensional, we show that a compact descriptor can be learnt from them using discriminative metric learning. This compact descriptor has a better recognition accuracy and is very well suited to large scale identification tasks."
http://videolectures.net/bmvc2013_zhang_preserving_projections/,"Linear projection for reducing data dimensionality is a common practice in various data processing applications. Among the existing projection methods, Principal Component Analysis (PCA) is arguably the most popular one. Standard PCA used in image preprocessing pursues the projection directions by minimizing the reconstruction error in a least square sense. However, since PCA does not adapt to the data or any specific domains, it may lead to severe loss of certain discriminative features during the projection, and damage the performance of either human perception (e.g. stimulus in the visual cortex, as modeled by Gabor wavelets), or machine perceptions (e.g. recognizing the images based on a certain type of visual features), or both. In this paper, we propose a novel Perception Preserving Projections (PPP) method to preserve the information for specific perception systems. In particular, PPP incorporates domain-specific feature extractor into the standard PCA formulation for the projection learning procedure. This enables PPP to make more sensible projections for feature based perception systems while retaining the simplicity and unsupervised manner of PCA. In experimental studies, PPP shows clear effectiveness and improvement over PCA in terms of two performance metrics: feature extraction deviation and the pattern recognition accuracy."
http://videolectures.net/bmvc2013_sharma_novel_approach/,"The kernel trick – commonly used in machine learning and computer vision – enables learning of non-linear decision functions without having to explicitly map the original data to a high dimensional space. However, at test time, it requires evaluating the kernel with each one of the support vectors, which is time consuming. In this paper, we propose a novel approach for learning non-linear SVM corresponding to the histogram intersection kernel without using the kernel trick. We formulate the exact non-linear problem in the original space and show how to perform classification directly in this space. The learnt classifier incorporates non-linearity while maintaining O(d) testing complexity (for d-dimensional input space), compared to O(d Nsv) when using the kernel trick. We show that the SVM problem with histogram intersection kernel is quasi-convex in input space and outline an iterative algorithm to solve it. The proposed approach has been validated in experiments where it is compared with other linear SVM-based methods, showing that the proposed method achieves similar or better performance at lower computational and memory costs."
http://videolectures.net/bmvc2013_amin_pictorial_structures/,"Pictorial structure models are the de facto standard for 2D human pose estimation. Numerous refinements and improvements have been proposed such as discriminatively trained body part detectors, flexible body models, and local and global mixtures. While these techniques allow to achieve state-of-the-art performance for 2D pose estimation, they have not yet been extended to enable pose estimation in 3D. This paper thus proposes a multi-view pictorial structures model that builds on recent advances in 2D pose estimation and incorporates evidence across multiple viewpoints to allow for robust 3D pose estimation. We evaluate our multi-view pictorial structures approach on the HumanEva-I and MPII Cooking dataset. In comparison to related work for 3D pose estimation our approach achieves similar or better results while operating on single-frames only and not relying on activity specific motion models or tracking. Notably, our approach outperforms state-of-the-art for activities with more complex motions."
http://videolectures.net/bmvc2013_brand_transitive/,"Person re-identification accuracy can be significantly improved given a training set that demonstrates changes in appearances associated with the two non-overlapping cameras involved. Here we test whether this advantage can be maintained when directly annotated training sets are not available for all camera-pairs at the site. Given the training sets capturing correspondences between cameras A and B and a different training set capturing correspondences between cameras B and C, the Transitive Re-IDentification algorithm (TRID) suggested here provides a classifier for (A;C) appearance pairs. The proposed method is based on statistical modeling and uses a marginalization process for the inference. This approach significantly reduces the annotation effort inherent in a learning system, which goes down from O(N2) to O(N), for a site containing N cameras. Moreover, when adding camera (N +1), only one inter-camera training set is required for establishing all correspondences. In our experiments we found that the method is effective and more accurate than the competing camera invariant approach."
http://videolectures.net/bmvc2013_pfister_pose_tracking/,"The objective of this work is to estimate upper body pose for signers in TV broadcasts. Given suitable training data, the pose is estimated using a random forest body joint detector. However, obtaining such training data can be costly. The novelty of this paper is a method of transfer learning which is able to harness existing training data and use it for new domains. Our contributions are: (i) a method for adapting existing training data to generate new training data by synthesis for signers with different appearances, and (ii) a method for personalising training data. As a case study we show how the appearance of the arms for different clothing, specifically short and long sleeved clothes, can be modelled to obtain person-specific trackers. We demonstrate that the transfer learning and person specific trackers significantly improve pose estimation performance."
http://videolectures.net/bmvc2013_burenius_part_recognition/,"This paper addresses the problem of human pose estimation, given images taken from multiple dynamic but calibrated cameras. We consider solving this task using a part-based model and focus on the part appearance component of such a model. We use a random forest classiﬁer to capture the variation in appearance of body parts in 2D images. The result of these 2D part detectors are then aggregated across views to produce consistent 3D hypotheses for parts. We solve correspondences across views for mirror symmetric parts by introducing a latent variable. We evaluate our part detectors qualitatively and quantitatively on a dataset gathered from a professional football game."
http://videolectures.net/bmvc2013_demetz_rank_transform/,"Most researchers agree that invariances are desirable in computer vision systems. However, one always has to keep in mind that this is at the expense of accuracy: By construction, all invariances inevitably discard information. The concept of morphological invariance is a good example for this trade-off and will be in the focus of this paper. Our goal is to develop a descriptor of local image structure that carries the maximally possible amount of local image information under this invariance. To fulfill this requirement, our descriptor has to encode the full ordering of the pixel intensities in the local neighbourhood. As a solution, we introduce the complete rank transform, which stores the intensity rank of every pixel in the local patch. As a proof of concept, we embed our novel descriptor in a prototypical TV􀀀L1-type energy functional for optical flow computation, which we minimise with a traditional coarse-to-fine warping scheme. In this straightforward framework, we demonstrate that our descriptor is preferable over related features that exhibit the same invariance. Finally, we show by means of public benchmark systems that our method produces - in spite of its simplicity - results of competitive quality."
http://videolectures.net/bmvc2013_stueckler_efficient_dense/,"Motion is a fundamental segmentation cue in video. Many current approaches segment 3D motion in monocular or stereo image sequences, mostly relying on sparse interest points or being dense but computationally demanding. We propose an efficient expectation-maximization (EM) framework for dense 3D segmentation of moving rigid parts in RGB-D video. Our approach segments two images into pixel regions that undergo coherent 3D rigid-body motion. Our formulation treats background and foreground objects equally and poses no further assumptions on the motion of the camera or the objects than rigidness. While our EM-formulation is not restricted to a specific image representation, we supplement it with efficient image representation and registration for rapid segmentation of RGB-D video. In experiments we demonstrate that our approach recovers segmentation and 3D motion at good precision."
http://videolectures.net/bmvc2013_zhu_dictionary_learning/,"We present a novel cross-dataset action recognition framework that utilizes relevant actions from other visual domains as auxiliary knowledge for enhancing the learning system in the target domain. The data distribution of relevant actions from a source dataset is adapted to match the data distribution of actions in the target dataset via a cross-domain discriminative dictionary learning method, through which a reconstructive, discriminative and domain-adaptive dictionary-pair can be learned. Using selected categories from the HMDB51 dataset as the source domain actions, the proposed framework achieves outstanding performance on the UCF YouTube dataset."
http://videolectures.net/bmvc2013_schulter_object_discovery/,"Unsupervised object discovery is the task of finding recurring objects over an unsorted set of images without any human supervision, which becomes more and more important as the amount of visual data grows exponentially. Existing approaches typically build on still images and rely on different prior knowledge to yield accurate results. In contrast, we propose a novel video-based approach, allowing also for exploiting motion information, which is a strong and physically valid indicator for foreground objects, thus, tremendously easing the task. In particular, we show how to integrate motion information in parallel with appearance cues into a common conditional random field formulation to automatically discover object categories from videos. In the experiments, we show that our system can successfully extract, group, and segment most foreground objects and is also able to discover stationary objects in the given videos. Furthermore, we demonstrate that the unsupervised learned appearance models also yield reasonable results for object detection on still images."
http://videolectures.net/bmvc2013_neubert_superpixles/,"There exist almost as many superpixel segmentation algorithms as applications they can be used for. So far, the choice of the right superpixel algorithm for the task at hand is based on their ability to resemble human-made ground truth segmentations (besides runtime and availability). We investigate the equally important question of how stable the segmentations are under image changes as they appear in video data. Further we propose a new quality measure that evaluates how well the segmentation algorithms cover relevant image boundaries. Instead of relying on human-made annotations, that may be biased by semantic knowledge, we present a completely data-driven measure that inherently emphasizes the importance of image boundaries. Our evaluation is based on two recently published datasets coming with ground truth optical flow fields. We discuss how these ground optical truth fields can be used to evaluate segmentation algorithms and compare several existing superpixel algorithms."
http://videolectures.net/bmvc2013_englebienne_gibbs_sampling/,"This paper proposes a novel probabilistic approach for appearance-based person reidentification in non-overlapping camera networks. It accounts for varying illumination, varying camera gain and has low computational complexity. More specifically, we present a graphical model where we model the person’s appearance in addition to camera illumination and gain. We analytically derive the solutions for the person’s appearance and camera properties, and use a novel constant time Gibbs sampling scheme to estimate the identification labels. We validate our algorithm on two indoor datasets and perform a comparative analysis with existing algorithms. We demonstrate significantly increased re-identification accuracy in addition to significantly reducing the computational complexity on our datasets."
http://videolectures.net/bmvc2013_feichtenhofer_scene_recognition/,"This paper presents spacetime forests defined over complementary spatial and temporal features for recognition of naturally occurring dynamic scenes. The approach improves on the previous state-of-the-art in both classification and execution rates. A particular improvement is with increased robustness to camera motion, where previous approaches have experienced difficulty. There are three key novelties in the approach. First, a novel spacetime descriptor is employed that exploits the complementary nature of spatial and temporal information, as inspired by previous research on the role of orientation features in scene classification. Second, a forest-based classifier is used to learn a multi-class representation of the feature distributions. Third, the video is processed in temporal slices with scale matched preferentially to scene dynamics over camera motion. Slicing allows for temporal alignment to be handled as latent information in the classifier and for efficient, incremental processing. The integrated approach is evaluated empirically on two publically available datasets to document its outstanding performance."
http://videolectures.net/bmvc2013_dubska_plane_mapping/,"This paper deals with the detection of orthogonal vanishing points. The aim is to efficiently cope with the clutter edges in real-life images and to determine the camera orientation in the Manhattan world reliably. We are using a modified scheme of the Cascaded Hough Transform where only one Hough space is accumulated – the space of the vanishing points. The parameterization of the vanishing points – the “diamond space” – is based on the PClines line parameterization and it is defined as a mapping of the whole real projective plane to a finite space. Our algorithm for detection of vanishing points operates directly on edgelets detected by an edge detector, skipping the common step of grouping edges into straight lines or line segments. This decreases the number of configuration parameters and reduces the complexity of the algorithm. Evaluated on the York Urban DB, our algorithm yields 98.04% success rate at 10 angular error tolerance, which outperforms comparable existing solutions. Our parameterization of vanishing points is in all aspects linear; it involves no goniometric or other non-linear operations and thus it is suitable for implementation in embedded chips and circuitry. The iterative search scheme allows for finding orthogonal triplets of vanishing points with high accuracy and low computational costs. At the same time, our approach can be used without the orthogonality constraint."
http://videolectures.net/bmvc2013_rodola_vector_extrapolation/,"We propose the adoption of a vector extrapolation technique to accelerate convergence of correspondence problems under the quadratic assignment formulation for attributed graph matching (QAP). In order to capture a broad range of matching scenarios, we provide a class of relaxations of the QAP under elastic net constraints. This allows us to regulate the sparsity/complexity trade-off which is inherent to most instances of the matching problem, thus enabling us to study the application of the acceleration method over a family of problems of varying difficulty. The validity of the approach is assessed by considering three different matching scenarios; namely, rigid and non-rigid three-dimensional shape matching, and image matching for Structure from Motion. As demonstrated on both real and synthetic data, our approach leads to an increase in performance of up to one order of magnitude when compared to the standard methods."
http://videolectures.net/bmvc2013_hofer_geometric_constraints/,"Generating accurate 3D models for man-made environments can be a challenging task due to the presence of texture-less objects or wiry structures. Since traditional point-based 3D reconstruction approaches may fail to integrate these structures into the resulting point cloud, a different feature representation is necessary. We present a novel approach which uses point features for camera estimation and additional line segments for 3D reconstruction. To avoid appearance-based line matching, we use purely geometric constraints for hypothesis generation and verification. Therefore, the proposed method is able to reconstruct both wiry structures as well as solid objects. The algorithm is designed to generate incremental results using online Structure-from-Motion and linebased 3D modelling in parallel. We show that the proposed method outperforms previous descriptor-less line matching approaches in terms of run-time while delivering accurate results."
http://videolectures.net/bmvc2013_lovegrove_spline_fusion/,"This paper describes a general continuous-time framework for visual-inertial simultaneous localization and mapping and calibration. We show how to use a spline parameterization that closely matches the torque-minimal motion of the sensor. Compared to traditional discrete-time solutions, the continuous-time formulation is particularly useful for solving problems with high-frame rate sensors and multiple unsynchronized devices. We demonstrate the applicability of the method for multi-sensor visual-inertial SLAM and calibration by accurately establishing the relative pose and internal parameters of multiple unsynchronized devices. We also show the advantages of the approach through evaluation and uniform treatment of both global and rolling shutter cameras within visual and visual-inertial SLAM systems."
http://videolectures.net/bmvc2013_hoppe_point_clouds/,"In this paper we propose a new method to incrementally extract a surface from a consecutively growing Structure-from-Motion (SfM) point cloud in real-time. Our method is based on a Delaunay triangulation (DT) on the 3D points. The core idea is to robustly label all tetrahedra into freeand occupied space using a random field formulation and to extract the surface as the interface between differently labeled tetrahedra. For this reason, we propose a new energy function that achieves the same accuracy as state-of-the-art methods but reduces the computational effort significantly. Furthermore, our new formulation allows us to extract the surface in an incremental manner, i. e. whenever the point cloud is updated we adapt our energy function. Instead of minimizing the updated energy with a standard graph cut, we employ the dynamic graph cut of Kohli et al. [1] which enables efficient minimization of a series of similar random fields by re-using the previous solution. In such a way we are able to extract the surface from an increasingly growing point cloud nearly independent of the overall scene size."
http://videolectures.net/bmvc2013_zienkiewicz_visual_odometry/,"We present a technique whereby a single camera can be used as a high precision visual odometry sensor in a range of practical settings using simple, computationally efficient techniques. Taking advantage of the local planarity of common floor surfaces, we use real-time dense alignment of a 30Hz video stream as the camera looks down from a fast-moving robot, making use of the whole texture available rather than sparse feature points. Our key novelty, and crucial to the practicality of this approach, is rapid and automatic calibration for 6DoF camera extrinsics relative to the robot frame. Our experiments show robust performance over a range of low-textured real surfaces."
http://videolectures.net/bmvc2013_chen_stereo_matching/,"Adaptive support weight (ASW) approach represents the state-of-the-art local stereo matching method. Recent extensive evaluation studies on ASW approaches show that the bilateral filter weight function enables outstanding performance on a large dataset in comparison with various weight functions. However, it does not resolve the ambiguity induced by nearby pixels at different disparities but with similar colors. In this paper, we propose a novel trilateral filter based ASW method which remedies such ambiguities by considering disparity discontinuities through color discontinuity boundaries, i.e., the strength of the boundary between two pixels. The experimental evaluation on the Middlebury benchmark shows that the proposed algorithm ranks 15th out of 150 submissions and is the current most accurate local stereo matching algorithm."
http://videolectures.net/bmvc2013_kuang_space_analysis/,"Discovering a latent common space between different modalities plays an important role in cross-modality pattern recognition. Existing techniques often require absolutelypaired observations as training data, and are incapable of capturing more general semantic relationships between cross-modality observations. This greatly limits their applications. In this paper, we propose a general framework for learning a latent common space from relatively-paired observations (i.e., two observations from different modalities are more-likely-paired than another two). Relative-pairing information is encoded using relative proximities of observations in the latent common space. By building a discriminative model and maximizing a distance margin, a projection function that maps observations into the latent common space is learned for each modality. Cross-modality pattern recognition can then be carried out in the latent common space. To evaluate its performance, the proposed framework has been applied to cross-pose face recognition and feature fusion. Experimental results demonstrate that the proposed framework outperforms other state-of-the-art approaches."
http://videolectures.net/bmvc2013_ge_image_retrieval/,"State-of-the-art image retrieval systems typically represent an image with a bag of low-level features. Since different images often exhibit different kinds of low-level characteristics, it is desirable to represent an image with multiple types of complementary features. The systems scalability is, however, significantly lowered when increasing the number of feature types, as the amount of data is also increased rapidly both in index and in query representation. In this paper, we apply sparse coding to derive a compact yet discriminative image representation from multiple types of features for large-scale image retrieval. We first convert each feature descriptor into a sparse code, and aggregate each type of sparsecoded features into a single vector by max-pooling. Multiple vectors from different types of features are then concatenated and compressed to obtain the final representation. Our approach allows us to add more types of features to improve discriminability without sacrificing scalability. In particular, we design a new micro feature which is complementary to existing local invariant features. By combining our micro feature with various local invariant features using the sparse-coding framework, our final compact representation outperforms the state of the art both in retrieval performance and in scalability."
http://videolectures.net/bmvc2013_mille_interactive_segmentation/,"Active contours and minimal paths have been extensively studied theoretical tools for image segmentation. The recent geodesically linked active contour model, which basically consists in a set of vertices connected by paths of minimal cost, blend the benets of both concepts. This makes up a closed piecewise-smooth curve, over which an edge or region energy functional can be formulated. As an important shortcom- ing, the geodesically linked active contour model in its initial formulation does not guarantee the curve to be simple, consistent with respect to the purpose of segmentation. In this paper, we propose to extract a relevant contour from a set of possible paths, such that the resulting structure ts the image data and is simple. Toward this goal, we introduce a novel term to favor the simplicity of the generated contour, as well as a local search method to choose the best combination among possible paths."
http://videolectures.net/bmvc2013_wu_nearest_points/,"Set based recognition has been attracting more and more attention in recent years, benefitting from two facts: the difficulty of collecting sets of images for recognition fades quickly, and set based recognition models generally outperform the ones for single instance based recognition. In this paper, we propose a novel model called collaboratively regularized nearest points (CRNP) for solving this problem. The proposal inherits the merits of simplicity, robustness, and high-efficiency from the very recently introduced regularized nearest points (RNP) method on finding the set-to-set distance using the l2-norm regularized affine hulls. Meanwhile, CRNP makes use of the powerful discriminative ability induced by collaborative representation, following the same idea as that in sparse recognition for classification (SRC) for image-based recognition and collaborative sparse approximation (CSA) for set-based recognition. However, CRNP uses l2-norm instead of the expensive l1-norm for coefficients regularization, which makes it much more efficient. Extensive experiments on five benchmark datasets for face recognition and person re-identification demonstrate that CRNP is not only more effective but also significantly faster than other state-of-the-art methods, including RNP and CSA."
http://videolectures.net/bmvc2013_browstow_something/,"We want algorithms that can tell us what went where in a video. Tracking is hard because each situation is different, featuring a different camera operator, and subject(s) whose appearance, motion, and setting are in a novel configuration. While each video may be pixel-wise unique, we hypothesize that the observed motions are quite similar, at least for tracking purposes. We propose that better tracking can be achieved by learning to automatically associate different videos (or parts) with different algorithms. Instead of seeking an elusive one-size-fits-all tracking strategy (often in the form of an energy function), we advocate keeping multiple strategies, but recognizing when/where to use each. We demonstrate this approach for the problems of optical flow and interest-point tracking."
http://videolectures.net/bmvc2013_rosten_optimized_corner/,"Many problems in computer vision involve optimization. Choosing what to optimize can be difficult; firstly because optimization of the appropriate objective may be intractably difficult and secondly because even the correct choice of objective may not be clear. This talk is about optimization in three areas of computer vision: corner detection, object detection and biological optical microscopy. A corner detector should repeatable detect the same corners between images, and ideally should operate efficiently. These objectives can be quantified, and I demonstrate a method for generating optimized corner detectors. In object detection, the definition of a detection versus a misdetection or missed detection is not obvious. On this subject, I will present an object detection system for detecting small objects. This system introduces a new family of features, and detectors optimized for several different definitions of what a detection really is. The third part of this talk is about about using factorial hidden Markov model analysis as an object detection strategy to break the resolution barrier in biological optical microscopy. By optimizing the correct model--an ensemble of fluorescent protein positions---a resolution of up to four times higher than the theoretical resolution limit for this technique can be achieved."
