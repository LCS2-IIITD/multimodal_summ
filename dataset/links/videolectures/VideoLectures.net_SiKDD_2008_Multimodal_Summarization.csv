Video_Presentation,Abstracts
http://videolectures.net/sikdd08_dali_tes/,"In this paper we present a machine learning approach to extract subject-predicate-object triplets from English sentences.  SVM is used to train a model on human annotated triplets, and the features are computed from three parsers."
http://videolectures.net/sikdd08_rusu_sgdt/,"Information nowadays has become more and more accessible, so much as to give birth to an information overload issue. Yet important decisions have to be made, depending on the available information. As it is impossible to read all the relevant content that helps one stay informed, a possible solution would be condensing data and obtaining the kernel of a text by automatically summarizing it. We present an approach to analyzing text and retrieving valuable information in the form of a semantic graph based on subject-verb-object triplets extracted from sentences. Once triplets have been generated, we apply several techniques in order to obtain the semantic graph of the document: coreference and anaphora resolution of named entities and semantic normalization of triplets. Finally, we describe the automatic document summarization process starting from the semantic representation of the text. The experimental evaluation carried out step by step on several Reuters newswire articles shows a comparable performance of the proposed approach with other existing methodologies. For the assessment of the document summaries we utilize an automatic summarization evaluation package, so as to show a ranking of various summarizers."
http://videolectures.net/sikdd08_rupnik_ssa/,"This paper is an overview of a recent approach for solving linear support vector machines (SVMs), the PEGASOS algorithm. The algorithm is based on a technique called the stochastic subgradient descent and employs it for solving the optimization problem posed by the soft margin SVM - a very popular classifier.  We briefly introduce the SVM problem and one of the widely used solvers, SVM light, then describe the PEGASOS algorithm and present some experiments. We conclude that the algorithm efficiently discovers suboptimal solutions to large scale problems within a matter of seconds."
http://videolectures.net/sikdd08_aleksovski_fpa/,"Distance-based algorithms for both clustering and prediction are popular within the machine learning community. These algorithms typically deal with attributevalue (single-table) data. The distance functions used are typically hard-coded. We are concerned here with generic distance-based learning algorithms that work on arbitrary types of structured data. In our approach, distance functions are not hard-coded, but are rather first-class citizens that can be stored, retrieved and manipulated. In particular, we can assemble, on-the-fly, distance functions for complex structured data types from pre-existing components. To implement the proposed approach, we use the strongly typed functional language Haskell. Haskell allows us to explicitly manipulate distance functions. We have produced a SW library/application with structured data types and distance functions and used it to evaluate the potential of Haskell as a basis for future work in the field of distancebased machine learning."
http://videolectures.net/sikdd08_rupnik_imt/,"Part-of-speech (PoS) or, better, morphosyntactic tagging is the process of assigning morphosyntactic categories to words in a text, an important pre-processing step for most human language technology applications. PoS-tagging of Slovene texts is a challenging task since the size of the tagset is over one thousand tags (as opposed to English, where the size is typically around sixty) and the state-of-the-art tagging accuracy is still below levels desired. The paper describes an experiment aimed at improving tagging accuracy for Slovene, by combining the outputs of two taggers – a proprietary rule-based tagger developed by the Amebis HLT company, and TnT, a tri-gram HMM tagger, trained on a handannotated corpus of Slovene. The two taggers have comparable accuracy, but there are many cases where, if the predictions of the two taggers differ, one of the two does assign the correct tag. We investigate training a classifier on top of the outputs of both taggers that predicts which of the two taggers is correct. We experiment with selecting different classification algorithms and constructing different feature sets for training and show that some cases yield a meta-tagger with a significant increase in accuracy compared to that of either tagger in isolation."
http://videolectures.net/sikdd08_novalija_eo/,"Ontologies are commonly used for annotating textual data mainly based on human language technologies [1]. This research focuses on manual extensions of ontologies to support the annotation of business news. Experiments were conducted on a well known Cyc ontology and using Cyc annotator on two business news datasets. We show that the proposed extensions of ontology results in annotation with better coverage of terms that are relevant for the business domain.  The results of identifying financial terms in business news using the original Cyc ontology show the average precision of 56% and recall of 41% in case of Reuters news and the average precision of 69% and the recall of 57% in case of Yahoo financial news. Using the proposed extension results with increased performance, the average precision of 82% and average recall of 73% for Yahoo financial news and average precision of 84% and average recall of 63% for Reuters news."
http://videolectures.net/sikdd08_fortuna_smtm/,"The variety of access and transport technologies available in modern computer networks pose significant challenges related to compatibility and quality of service (QoS) related issues. Applications and services can have many different and unique requirements towards the transportation services (TSs) they use to interconnect. Traditionally, applications are required to specify their QoS requirements in the language which the TSs understand. This results in reformulation of intuitive parameters (i.e. desired video resolution) to parameters understood by the TSs (i.e. required bandwidth). paper presents techniques for (a) automatic matchmaking of application requirements to the offers by TSs providers and (b) automatic translation of application requirements into the TSs QoS requirements. To this end semantic technologies, namely OpenCyc, are used for ontological modeling, translation and matchmaking. We present relevant examples on how semantic technologies can be used in the context of communication networks."
http://videolectures.net/sikdd08_dimitrovski_hami/,"In this paper, we describe an approach for the automatic medical annotation task of the 2008 CLEF cross-language image retrieval campaign (ImageCLEF). The data comprise 12076 fully annotated images according to the IRMA code. This work is focused on the process of feature extraction from images and hierarchical multi-label classification. To extract features from the images we used a technique called: local distribution of edges.  With this techniques each image was described with 80 variables. The goal of the classification task was to classify an image according to the IRMA code. The IRMA code is organized hierarchically. Hence, as classifer we selected an extension of the predictive clustering trees (PCTs) that is able to handle this type of data.  Further more, we constructed ensembles (Bagging and Random Forests) that use PCTs as base classifiers."
http://videolectures.net/sikdd08_bullas_si/,"Over three hundred four-second / 40hz time series datasets (from simulated emergency braking manoeuvres at English fatal accident sites and field trials) were classified using key characteristics of the braking sequences extracted for each event. These characteristics were then tested for significant difference between road surface types and braking system types. One key marker, average deceleration, was also compared against existing benchmark ‘typical’ values for acceptable performance as found in the literature."
http://videolectures.net/sikdd08_popovic_cpm/,"The paper presents model based on fuzzy methods for churn prediction in retail banking. The study was done on the real, anonymised data of 5000 clients of a retail bank. Real data are great strength of the study, as a lot of studies often use old, irrelevant or artificial data. Canonical discriminant analysis was applied to reveal variables that provide maximal separation between clusters of churners and non-churners. Combination of standard deviation, canonical discriminant analysis and k-means clustering results were used for outliers detection. Due to the fuzzy nature of practical customer relationship management problems it was expected, and shown, that fuzzy methods performed better than the classical ones. According to the results of the preliminary data exploration and fuzzy clustering with different values of the input parameters for fuzzy c-means algorithm, the best parameter combination was chosen and applied to training data set. Four different prediction models, called prediction engines, have been developed. The definitions of clients in the fuzzy transitional conditions and the distance of k instances fuzzy sums were introduced. The prediction engine using these sums performed best in churn prediction, applied to both balanced and non-balanced test sets."
