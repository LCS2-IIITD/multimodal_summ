Video_Presentation,Abstracts
http://videolectures.net/ecmlpkdd08_pedersen_emlu/,"The tutorial on embedded machine learning will present a case study of implementing and using a binary support vector machine in wireless sensor networks. It will use a very popular operating system for wireless sensor networks called TinyOS and the new exciting open hardware/software platform Lego Mindstorms NXT from LEGO. Outline of the tutorial (structured list of topics): The tutorial provides an overview of embedded machine learning and an Overview of wireless sensor networks, TinyOS and the programming language nesC. It provides an introduction to LEGO MINDSTORMS and the main hardware items needed to engage the problem in a meaningful way. The mapping of the binary support vector machine to the constraints of the embedded machine learning problem is given (memory, battery, little CPU)."
http://videolectures.net/ecmlpkdd08_bottcher_kdfe/,"Data mining has traditionally concentrated on the analysis of a static world, in which data instances are collected, stored and analyzed to derive models and take decisions according to them. More recent research on stream mining has put forward the need to deal with data that cannot be collected and stored statically but must be analyzed on the fly. At the same time, the need to store, maintain, query and update models derived from the data has been recognized and advocated [LT08]. However, these are only two aspects of the dynamic world that must be analyzed with data mining: The world is changing and so do the accumulating data and, ultimately, the models derived from them. The challenges for Knowledge Discovery in a changing world have two forms: (a) adapting the patterns to the changes in the population and (b) capturing, understanding and highlighting the changes. In this tutorial, we discuss the topics associated with data mining for changing environments and elaborate on research advances in this area. Relevant research comes among else from the fields of incremental mining, stream mining, temporal mining and change detection. Since this is a very wide field, we concentrate on the second challenge, the understanding of change, and we organize research contributions in this context."
http://videolectures.net/ecmlpkdd08_lazarevic_dmfa/,"Anomaly detection corresponds to discovery of events that typically do not conform to expected normal behavior. Such events are often referred to as anomalies, outliers, exceptions, deviations, aberrations, surprise, peculiarities or contaminants in different application domains Detection of anomalies is a common problem in many domains, such as detecting fraudulent credit card transactions, insurance and tax fraud detection, intrusion detection for cyber security, failure detection, direct marketing, and medical diagnostics. Although anomalies are by definition infrequent, in many examples their importance is quite high compared to other events, making their detection extremely important. This tutorial will provide an overview of the research done in the increasingly important field of anomaly detection. The tutorial will cover the existing literature from a variety of perspectives, such as nature of input/output, and the availability of supervision. Anomalies will be divided into three broad groups: (i) Point anomalies, (ii) Contextual anomalies, and (iii) Structural anomalies, and a wide variety of anomaly detection methods appropriate for each type of anomaly will be presented. Additionally, the tutorial will discuss several application domains, such as intrusion detection, fraud detection, industrial damage detection, healthcare informatics, where anomaly detection plays a central role."
http://videolectures.net/ecmlpkdd08_gasso_trfim/,"Machine Learning algorithms often involve the joint optimization of several objective functions for achieving good generalization performance. Well known examples are Support Vector Machines for regression, classification and novelty detection or the Lasso problem where one objective function is related to the perfect fit of the data and the second one concerns particular desirable properties such as smoothness or sparsity of the target model. These two goals being antagonist, a trade-off needs to be achieved. Hence, the learning process can be cast in a multi-objective optimisation problem. The aim of this tutorial is to bridge the gap between the multi-objective optimization literature and the machine learning community by providing an insight on the Pareto frontier, the efficient computation of this frontier using regularization path algorithms. The connection between these algorithms and parametric optimisation problems will be highlighted as well as issues related to sparsity, model selection and numerical implementation."
http://videolectures.net/ecmlpkdd08_jain_dcyb/,"The practice of classifying objects according to perceived similarities is the basis for much of science. Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms in to taxonomic ranks: domain, kingdom, phylum, class, etc.). Cluster analysis is the formal study of algorithms and methods for grouping objects according to measured or perceived intrinsic characteristics. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes cluster analysis (unsupervised learning) from discriminant analysis (supervised learning). The objective of cluster analysis is to simply find a convenient and valid organization of the data, not to establish rules for separating future data into categories. The development of clustering methodology has been a truly interdisciplinary endeavor. Taxonomists, social scientists, psychologists, biologists, statisticians, engineers, computer scientists, medical researchers, and others who collect and process real data have all contributed to clustering methodology. According to JSTOR, data clustering first appeared in the title of a 1954 article dealing with anthropological data. One of the most well-known, simplest and popular clustering algorithms is K-means. It was independently discovered by Steinhaus (1955), Lloyd (1957), Ball and Hall (1965) and McQueen (1967)! A search via Google Scholar found 22,000 entries with the word clustering and 1,560 entries with the words data clustering in 2007 alone. Among all the papers presented at CVPR, ECML, ICDM, ICML, NIPS and SDM in 2006 and 2007, 150 dealt with clustering. This vast literature speaks to the importance of clustering in machine learning, data mining and pattern recognition. A cluster is comprised of a number of similar objects grouped together. While it is easy to give a functional definition of a cluster, it is very difficult to give an operational definition of a cluster. This is because objects can be grouped into clusters with different purposes in mind. Data can reveal clusters of different shapes and sizes. Thus the crucial problem in identifying clusters in data is to specify or learn a similarity measure. In spite of thousands of clustering algorithms that have been published, a user still faces a dilemma regarding the choice of algorithm, distance metric, data normalization, number of clusters, and validation criteria. A familiarity with the application domain and clustering goals will certainly help in making an intelligent choice. This talk will provide background, discuss major challenges and key issues in designing clustering algorithms, summarize well known clustering methods, and point out some of the emerging research directions, including semi-supervised clustering that exploits pairwise constraints, ensemble clustering that combines results of multiple clusterings, learning distance metrics from side information, and simultaneous feature selection and clustering."
http://videolectures.net/ecmlpkdd08_mooney_llfi/,"Current systems that learn to process natural language require laboriously constructed human-annotated training data. Ideally, a computer would be able to acquire language like a child by being exposed to linguistic input in the context of a relevant but ambiguous perceptual environment. As a step in this direction, we present a system that learns to sportscast simulated robot soccer games by example. The training data consists of textual human commentaries on Robocup simulation games. A set of possible alternative meanings for each comment is automatically constructed from game event traces. Our previously developed systems for learning to parse and generate natural language (KRISP and WASP) were augmented to learn from this data and then commentate novel games. The system is evaluated based on its ability to parse sentences into correct meanings and generate accurate descriptions of game events. Human evaluation was also conducted on the overall quality of the generated sportscasts and compared to human-generated commentaries."
http://videolectures.net/ecmlpkdd08_soulie_idmc/,"Business Intelligence is a very active sector in all industrial domains. Classical techniques (reporting and Olap), mainly concerned with presenting data, are already widely deployed. Meanwhile, Data Mining has long been used in companies as a niche-technique, reserved for experts only and for very specific problems (credit scoring, fraud detection for example). But with the increasing availability of large data volumes (in particular, but not only, from the Web), companies are more and more turning to data mining to provide them with high added-value predictive analytics. However producing models in large numbers, making use of large data volumes in an industrial context can only happen if solutions to challenges, both theoretic and operational, are found: we need algorithms which can be used to produce models when datasets have thousands of variables and millions of observations; we need to learn how to run and control the correct execution of hundreds of models; we need ways to automate the data mining process. I will present these constraints in industrial contexts and show how KXEN has exploited theoretical results (coming from Vladimir Vapnik's work) to provide answers to the above-mentioned challenges. I will give a few examples of real-life applications and will conclude with some remarks on the future of data mining in the industrial domain."
http://videolectures.net/ecmlpkdd08_ramakrishnan_trie/,"In a broad range of data mining tasks, the fundamental challenge is to efficiently explore a very large space of alternatives. The difficulty is two-fold: first, the size of the space raises computational challenges, and second, it can introduce data sparsity issues even in the presence of very large datasets. In this talk, we'll consider how the use of hierarchies (e.g., taxonomies, or the OLAP multi-dimensional model) can help mitigate the problem."
http://videolectures.net/ecmlpkdd08_freund_mymcp/,"The advance of fluorescent tagging and of confocal microscopy is allowing biologists to image biochemical processes at a level of detail that was unimaginable just a few years ago. However, as the analysis of these images is done mostly by hand, there is a severe bottleneck in transforming these images into useful quantitative data that can be used to evaluate mathematical models. One of the inherent challenges involved in automating this transformation is that image data is highly variable. This requires a recalibration of the image processing algorithms for each experiment. We use machine learning methods to enable the experimentalist to calibrate the image processing methods without having any knowledge of how these methods work. This, we believe, will allow the rapid integration of computer vision methods with confocal microscopy and open the way to the development of quantitative spatial models of cellular processes. For more information, see the Bio-medical image analysis page"
http://videolectures.net/ecmlpkdd08_nguyen_icwpc/,"In this paper, we address the semi-supervised learning problem when there is a small amount of labeled data augmented with pairwise constraints indicating whether a pair of examples belongs to a same class or different classes. We introduce a discriminative learning approach that incorporates pairwise constraints into the conventional margin-based learning framework. We also present an efficient algorithm, PCSVM, to solve the pairwise constraint learning problem. Experiments with 15 data sets show that pairwise constraint information significantly increases the performance of classification."
http://videolectures.net/ecmlpkdd08_brefeld_eaai/,"Training processes of structured prediction models such as structural SVMs involve frequent computations of the maximum-a-posteriori (MAP) prediction given a parameterized model. For specific output structures such as sequences or trees, MAP estimates can be computed efficiently by dynamic programming algorithms such as the Viterbi algorithm and the CKY parser. However, when the output structures can be arbitrary graphs, exact calculation of the MAP estimate is an NP-complete problem. In this paper, we compare exact inference and approximate inference for labeling graphs. We study the exact junction tree and the approximate loopy belief propagation and sampling algorithms in terms of performance and ressource requirements."
http://videolectures.net/ecmlpkdd08_do_afmft/,"We propose a new algorithm for training a linear Support Vector Machine in the primal. The algorithm mixes ideas from non smooth optimization, subgradient methods, and cutting planes methods. This yields a fast algorithm that compares well to state of the art algorithms. It is proved to require $O(1/{lambdaepsilon})$ iterations to converge to a solution with accuracy $epsilon$. Additionally we provide an exact shrinking method in the primal that allows reducing the complexity of an iteration to much less than $O(N)$ where $N$ is the number of training samples."
http://videolectures.net/ecmlpkdd08_gopalkrishnan_crip/,"The goal of distributed learning in P2P networks is to achieve results as close as possible to those from centralized approaches. Learning models of classification in a P2P network faces several challenges like scalability, peer dynamism, asynchronism and data privacy preservation. In this paper, we study the feasibility of building SVM classifiers in a P2P network. We show how cascading SVM can be mapped to a P2P network of data propagation. Our proposed P2P SVM provides a method for constructing classifiers in P2P networks with classification accuracy comparable to centralized classifiers and better than other distributed classifiers. The proposed algorithm also satisfies the characteristics of P2P computing and has an upper bound on the communication overhead. Extensive experimental results confirm the feasibility and attractiveness of this approach."
http://videolectures.net/ecmlpkdd08_bordes_slst/,"This paper proposes an online solver of the dual formulation of support vector machines for structured output spaces. We apply it to sequence labelling using the exact and greedy inference schemes. In both cases, the per-sequence training time is the same as a perceptron based on the same inference procedure, up to a small multiplicative constant. Comparing the two inference schemes, the greedy version is much faster. It is also amenable to higher order Markov assumptions and performs similarly on test. In comparison to existing algorithms, both versions match the accuracies of batch solvers that use exact inference after a single pass over the training examples."
http://videolectures.net/ecmlpkdd08_miettinen_tbcac/,"Matrix decompositions are used for many data mining purposes. One of these purposes is to find a concise but interpretable representation of a given data matrix. Different decomposition formulations have been proposed for this task, many of which assume a certain property of the input data (e.g., nonnegativity) and aim at preserving that property in the decomposition. In this paper we propose new decomposition formulations for binary matrices, namely the Boolean CX and CUR decompositions. They are natural combinations two previously presented decomposition formulations. We consider also two subproblems of these decompositions and present a rigorous theoretical study of the subproblems. We give algorithms for the decompositions and for the subproblems, and study their performance via extensive experimental evaluation. We show that even simple algorithms can give accurate and intuitive decompositions of real data, thus demonstrating the power and usefulness of the proposed decompositions."
http://videolectures.net/ecmlpkdd08_singh_auvo/,"We present a unified view of matrix factorization that frames the differences among popular methods, such as NMF, Weighted SVD, E-PCA, MMMF, pLSI, pLSI-pHITS, Bregman co-clustering, and many others, in terms of a small number of modeling choices. Many of these approaches can be viewed as minimizing a generalized Bregman divergence, and we show that (i) a straightforward alternating projection algorithm can be applied to almost any model in our unified view; (ii) the Hessian for each projection has special structure that makes a Newton projection feasible, even when there are equality constraints on the factors, which allows for matrix co-clustering; and (iii) alternating projections can be generalized to simultaneously factor a set of matrices that share dimensions. These observations immediately yield new optimization algorithms for the above factorization methods, and suggest novel generalizations of these methods such as incorporating row/column biases, and adding or relaxing clustering constraints."
http://videolectures.net/ecmlpkdd08_karatzoglou_immm/,"Collaborative filtering is a popular method for personalizing product recommendations. Maximum Margin Matrix Factorization (MMMF) has been proposed as one successful learning approach to this task and has been recently extended to structured ranking losses. In this paper we discuss a number of extensions to MMMF by introducing offset terms, item dependent regularization and a graph kernel on the recommender graph. We show equivalence between graph kernels and the recent MMMF extensions by Mnih and Salakhutdinov. Experimental evaluation of the introduced extensions showimproved performance over the original MMMF formulation."
http://videolectures.net/ecmlpkdd08_cao_lbsf/,"Memory-based collaborative filtering aims at predicting the utility of a certain item for a particular user based on the previous ratings from similar users and similar items. Previous studies in finding similar users and items are based on user-defined similarity metrics such as Pearson Correlation Coefficient or Vector Space Similarity which are not adaptive and optimized for different applications and datasets. Moreover, previous studies have treated the similarity function calculation between users and items separately. In this paper, we propose a novel adaptive bidirectional similarity metric for collaborative filtering. We automatically learn similarities between users and items simultaneously through matrix factorization. We show that our model naturally extends the memory based approaches. Theoretical analysis shows our model to be a novel generalization of the SVD model. We evaluate our method using three benchmark datasets, including MovieLens, EachMovie and Netflix, through which we show that our methods outperform many previous baselines."
http://videolectures.net/ecmlpkdd08_eichinger_mewc/,"An important problem in software engineering is the automated discovery of noncrashing occasional bugs. In this work we address this problem and show that mining of weighted call graphs of program executions is a promising technique. We mine weighted graphs with a combination of structural and numerical techniques. More specifically, we propose a novel reduction technique for call graphs which introduces edge weights. Then we present an analysis technique for such weighted call graphs based on graph mining and on traditional feature selection schemes. The technique generalises previous graph mining approaches as it allows for an analysis of weights. Our evaluation shows that our approach finds bugs which previous approaches cannot detect so far. Our technique also doubles the precision of finding bugs which existing techniques can already localise in principle."
http://videolectures.net/ecmlpkdd08_leman_emm/,"In most databases, it is possible to identify small partitions of the data where the observed distribution is notably different from that of the database as a whole. In classical subgroup discovery, one considers the distribution of a single nominal attribute, and exceptional subgroups show a surprising increase in the occurrence of one of its values. In this paper, we introduce Exceptional Model Mining (EMM), a framework that allows for more complicated target concepts. Rather than finding subgroups based on the distribution of a single target attribute, EMM finds subgroups where a model fitted to that subgroup is somehow exceptional. We discuss regression as well as classification models, and define quality measures that determine how exceptional a given model on a subgroup is. Our framework is general enough to be applied to many types of models, even from other paradigms such as association analysis and graphical modeling."
http://videolectures.net/ecmlpkdd08_grosskreutz_toef/,"Subgroup discovery is the task of finding subgroups of a population which exhibit both distributional unusualness and high generality. Due to the non monotonicity of the corresponding evaluation functions, standard pruning techniques cannot be used for subgroup discovery, requiring the use of optimistic estimate techniques instead. So far, however, optimistic estimate pruning has only been considered for the extremely simple case of a binary target attribute and up to now no attempt was made to move beyond suboptimal heuristic optimistic estimates. In this paper, we show that optimistic estimate pruning can be developed into a sound and highly effective pruning approach for subgroup discovery. Based on a precise definition of optimality we show that previous estimates have been tight only in special cases. Thereafter, we present tight optimistic estimates for the most popular binary and multi-class quality functions, and present a family of increasingly efficient approximations to these optimal functions. As we show in empirical experiments, the use of our newly proposed optimistic estimates can lead to a speed up of an order of magnitude compared to previous approaches."
http://videolectures.net/ecmlpkdd08_wang_mmlf/,"Several manifold learning techniques have been developed to learn, given a data, a single lower dimensional manifold providing a compact representation of the original data. However, for complex data sets containing multiple manifolds of possibly of different dimensionalities, it is unlikely that the existing manifold learning approaches can discover all the interesting lower-dimensional structures. We therefore introduce a hierarchical manifolds learning framework to discover a variety of the underlying low dimensional structures. The framework is based on hierarchical mixture latent variable model, in which each submodel is a latent variable model capturing a single manifold. We propose a novel multiple manifold approximation strategy used for the initialization of our hierarchical model.The technique is first verified on artificial data with mixed 1-, 2- and 3-dimensional structures. It is then used to automatically detect lower-dimensional structures in disrupted satellite galaxies."
http://videolectures.net/ecmlpkdd08_waterschool_frmi/,"ATOS Worldline performs Fraud Risk Management on a wide range of products like Debit and Credit Cards, for Issuers, Acquirers and Petrol Companies, accross all fraud types like Theft, Counterfeit, and Internet Fraud. During our talk, we will highlight the different components needed for efficient Fraud Risk Management like creation of awareness, an efficient and flexible infrastructure, the right skills, the right support, etc. We will particularly focus of the role and position of data mining in this process and the relation between expert and data driven fraud detection. All will be amply illustrated with real fraud case examples."
http://videolectures.net/ecmlpkdd08_naudts_suiar/,"MDC Partners is based in Belgium and delivers business and market intelligence to pharma and medical device companies. As heterogenous public databases and the internet are our main data sources, semantic unification of concepts is the necessary driving force behind our data gathering en data storage platforms. In this talk we discuss a number of design philosophies and technical challenges in building a semantic data gathering, mining and reporting engine. Specifically on the semantic unification side, we will discuss alternative schemes for semantisizing text fragments, based on bootstrapped probabilistic grammars, reference ontologies and factual semantic data. The impact on the total semantic engine, from incoming data to reported queries, of alternative approaches will be discussed."
http://videolectures.net/ecmlpkdd08_cieslak_ldtf/,"Learning from unbalanced datasets presents a convoluted problem in which traditional learning algorithms may perform poorly. The objective functions used for learning the classifiers typically tend to favor the larger, less important classes in such problems. This paper compares the performance of several popular decision tree splitting criteria ? information gain, Gini measure, and DKM ? and identifies a new skew insensitive measure in Hellinger distance. We outline the strengths of Hellinger distance in class imbalance, proposes its application in forming decision trees, and performs a comprehensive comparative analysis between each decision tree construction method. In addition, we consider the performance of each tree within a powerful sampling wrapper framework to capture the interaction of the splitting metric and sampling. We evaluate over this wide range of datasets and determine which operate best under class imbalance."
http://videolectures.net/ecmlpkdd08_hempstalk_ocbc/,"One-class classification has important applications such as outlier and novelty detection. It is commonly tackled using density estimation techniques or by adapting a standard classification algorithm to the problem of carving out a decision boundary that describes the location of the target data. In this paper we investigate a simple method for one-class classification that combines the application of a density estimator, used to form a reference distribution, with the induction of a standard model for class probability estimation. In this method, the reference distribution is used to generate artificial data that is employed to form a second, artificial class. In conjunction with the target class, this artificial class is the basis for a standard two-class learning problem. We explain how the density function of the reference distribution can be combined with the class probability estimates obtained in this way to form an adjusted estimate of the density function of the target class. Using UCI datasets, and data from a typist recognition problem, we show that the combined model, consisting of both a density estimator and a class probability estimator, can improve on using either component technique alone when used for one-class classification. We also compare the method to one-class classification using support vector machines."
http://videolectures.net/ecmlpkdd08_kujala_rtuo/,"We study the problem of finding the most uniform partition of a label distribution on an interval. This problem occurs, e.g., in discretization of continuous features, where evaluation heuristics need to find the location of the best place to split the current feature. The weighted average of empirical entropies of the interval label distributions is often used for this task. We observe that this rule is sub-optimal, because it prefers short intervals too much. Therefore, we proceed to study alternative approaches. A solution that is based on compression turns out to be the best in our empirical experiments. We also study how these alternative methods affect the performance of classification algorithms."
http://videolectures.net/ecmlpkdd08_pietramala_agaf/,"This paper presents a Genetic Algorithm, called Olex-GA, for the induction of rule-based text classifiers of the form ``classify document $d$ under category $c$ if $t_1 in d$ or ... or $t_n in d$ and not ($t_{n+1} in d$ or ... or $t_{n+m} in d$) holds'', where each $t_i$ is a term. Olex-GA relies on an efficient emph{several-rules-per-individual} binary representation and uses the $F$-measure as the fitness function. The proposed approach is tested over the standard test sets Reuters and Ohsumed and compared against several classification algorithms (namely, Naive Bayes, Ripper, C4.5, SVM). Experimental results demonstrate that it achieves very good performance on both data collections, showing to be competitive with (and indeed outperforming in some cases) the evaluated classifiers. Note:  A prototype of the rule induction system Olex-GA described in that paper is available at the address http://www.mat.unical.it/Olex-GA"
http://videolectures.net/ecmlpkdd08_lin_ajtap/,"Polarizing discussions on political and social issues are common in mass and user-generated media. However, computer-based understanding of ideological discourse has been considered too difficult to undertake. In this paper we propose a statistical model for ideology discourse. By ideology we mean ``a set of general beliefs socially shared by a group of people.'' For example, Democratic and Republican are two major political ideologies in the United States. The proposed model captures lexical variations due to an ideological text's topic and due to an author or speaker's ideological perspective. To cope with the non-conjugacy of the logistic-normal prior we derived a variational inference algorithm for the model. We evaluate the proposed model on synthetic data as well as a written and a spoken political discourse. Experimental results strongly support that ideological perspectives are reflected in lexical variations."
http://videolectures.net/ecmlpkdd08_nakamura_tmlal/,"Polarizing discussions on political and social issues are common in mass and user-generated media. However, computer-based understanding of ideological discourse has been considered too difficult to undertake. In this paper we propose a statistical model for ideology discourse. By ideology we mean ``a set of general beliefs socially shared by a group of people.'' For example, Democratic and Republican are two major political ideologies in the United States. The proposed model captures lexical variations due to an ideological text's topic and due to an author or speaker's ideological perspective. To cope with the non-conjugacy of the logistic-normal prior we derived a variational inference algorithm for the model. We evaluate the proposed model on synthetic data as well as a written and a spoken political discourse. Experimental results strongly support that ideological perspectives are reflected in lexical variations."
http://videolectures.net/ecmlpkdd08_wang_ajsa/,"This paper introduces an approach which jointly performs a cascade of segmentation and labeling subtasks for Chinese lexical analysis, including word segmentation, named entity recognition and part-of-speech tagging. Unlike the traditional pipeline manner, the cascaded subtasks are conducted in a single step simultaneously, therefore error propagation could be avoided and the information could be shared among multi-level subtasks. In this approach, Weighted Finite State Transducers (WFSTs) are adopted. Within the unified framework of WFSTs, the models for each subtask are represented and then combined into a single one. Thereby, through one-pass decoding the joint optimal outputs for multi-level processes will be reached. The experimental results show the effectiveness of the presented joint processing approach, which significantly outperforms the traditional method in pipeline style."
http://videolectures.net/ecmlpkdd08_carlson_bief/,"We consider the problem of extracting structured records from semi-structured web pages with no human supervision required for each target web site. Previous work on this problem has either required significant human effort for each target site or used brittle heuristics to identify semantic data types. Our method only requires annotation for a few pages from a few sites in the target domain. Thus, after a tiny investment of human effort, our method allows automatic extraction from potentially thousands of other sites within the same domain. Our approach extends previous methods for detecting data fields in semi-structured web pages by matching those fields to domain schema columns using robust models of data values and contexts. Annotating 2-5 pages for 4-6 web sites yields an extraction accuracy of 83.8% on job offer sites and 91.1% on vacation rental sites. These results significantly outperform a baseline approach."
http://videolectures.net/ecmlpkdd08_ratle_lsct/,"We present a new framework for large-scale data clustering. The main idea is to modify functional dimensionality reduction techniques to directly optimize over discrete labels using stochastic gradient descent. Compared to methods like spectral clustering our approach solves a single optimization problem, rather than an ad-hoc two-stage optimization approach, does not require a matrix inversion, can easily encode prior knowledge in the set of implementable functions, and does not have an out-of-sample problem. Experimental results on both artificial and real-world datasets show the usefulness of our approach."
http://videolectures.net/ecmlpkdd08_kivinen_mbcw/,"Two recent breakthroughs have dramatically improved the scope and performance of k-means clustering: squared Euclidean seeding for the initialization step, and Bregman clustering for the iterative step. In this paper, we first unite the two frameworks by generalizing the former improvement to Bregman seeding - a biased randomized seeding technique using Bregman divergences - while generalizing its important theoretical approximation guarantees as well. We end up with a complete Bregman hard clustering algorithm integrating the distortion at hand in both the initialization and iterative steps. Our second contribution is to further generalize this algorithm to handle mixed Bregman distortions, which smooth out the asymetricity of Bregman divergences. In contrast to some other symmetrization approaches, our approach keeps the algorithm simple and allows us to generalize theoretical guarantees from regular Bregman clustering."
http://videolectures.net/ecmlpkdd08_rodrigues_cdsd/,"In this work we study the problem of continuously maintain a cluster structure over the data points generated by a sensor network. We propose DGClust, a new distributed algorithm which reduces both the dimensionality and the communication burdens, by allowing each local sensor to keep an online discretization of its data stream. Each new data point triggers a cell in this univariate grid, reflecting the current state of the data stream at the local site. Whenever a local site changes its state, it notifies the central server about the new state it is in. The central site keeps a small list of counters of the most frequent global states. A simple adaptive partitional clustering algorithm is applied to the frequent states central points, providing an anytime definition of the clusters centers. The approach is evaluated in the context of distributed sensor networks, presenting empirical and theoretical evidence of its advantages."
http://videolectures.net/ecmlpkdd08_zhang_dswap/,"This paper proposed StrAP (Streaming AP), extending Affinity Propagation (AP) to data steaming. AP, a new clustering algorithm, extracts the data items, or exemplars, that best represent the dataset using a message passing method. Several steps are made to build StrAP. The first one (Weighted AP) extends AP to weighted items with no loss of generality. The second one (Hierarchical WAP) is concerned with reducing the quadratic AP complexity, by applying AP on data subsets and further applying Weighted AP on the exemplars extracted from all subsets. Finally StrAP extends Hierarchical WAP to deal with changes in the data distribution. Experiments on artificial datasets, on the Intrusion Detection benchmark (KDD99) and on a real-world problem, clustering the stream of jobs submitted to the EGEE grid system, provide a comparative validation of the approach."
http://videolectures.net/ecmlpkdd08_gutmann_plip/,"We introduce the problem of learning the parameters of the probabilistic database ProbLog. Given the observed success probabilities of a set of queries, we compute the probabilities attached to facts that have a low approximation error on the training examples as well as on unseen examples. Assuming Gaussian error terms on the observed success probabilities, this naturally leads to a least squares optimization problem. Our approach, called LeProbLog, is able to learn both from queries and from proofs and even from both simultaneously. This makes it flexible and allows faster training in domains where the proofs are available. Experiments on real world data show the usefulness and effectiveness of this least squares calibration of probabilistic databases."
http://videolectures.net/ecmlpkdd08_thon_asmf/,"Artificial intelligence aims at developing agents that learn and act in complex environments. Realistic environments typically feature a variable number of objects, relations amongst them, and non-deterministic transition behavior. Standard probabilistic sequence models provide efficient inference and learning techniques, but typically cannot fully capture the relational complexity. On the other hand, statistical relational learning techniques are often too inefficient. In this paper, we present a simple model that occupies an intermediate position in this expressiveness/efficiency trade-off. It is based on CP-logic, an expressive probabilistic logic for modeling causality. However, by specializing CP-logic to represent a probability distribution over sequences of relational state descriptions, and employing a Markov assumption, inference and learning become more tractable and effective. We show that the resulting model is able to handle probabilistic relational domains with a substantial number of objects and relations."
http://videolectures.net/ecmlpkdd08_kok_esnft/,"Extracting knowledge from text has long been a goal of AI. Initial approaches were purely logical and brittle. More recently, the availability of large quantities of text on the Web has led to the development of machine learning approaches. However, to date these have mainly extracted ground facts, as opposed to general knowledge. Other learning approaches can extract logical forms, but require supervision and do not scale. In this paper we present an unsupervised approach to extracting semantic networks from large volumes of text. We use the TextRunner system [1] to extract tuples from text, and then induce general concepts and relations from them by jointly clustering the objects and relational strings in the tuples. Our approach is defined in Markov logic using four simple rules. Experiments on a dataset of two million tuples show that it outperforms three other relational clustering approaches, and extracts meaningful semantic networks."
