Video_Presentation,Abstracts
http://videolectures.net/bmvc2012_jegou_image_search/,"The first part of this tutorial, dedicated to large-scale image retrieval, will first introduce the typical use-cases and the datasets used for evaluation of image search when considering an unsupervised framework. We will present different classes of techniques considering different trade-offs with respect to efficiency and search quality. Starting with the most costly but precise patch-based matching and spatial verification techniques, we will present the bag-of-words model, its matching interpretation and several improvements, including re-ranking techniques based on spatial verification and query expansion. Finally, the most scalable techniques based on aggregation/coding techniques and compressed-domain search will be detailed."
http://videolectures.net/bmvc2012_kohli_discrete_models/,"Many problems in Computer Vision are formulated in form of a random filed of discrete variables. Examples range from low-level vision such as image segmentation, optical flow and stereo reconstruction, to high-level vision such as object recognition. The goal is typically to infer the most probable values of the random variables, known as Maximum a Posteriori (MAP) estimation. This has been widely studied in several areas of Computer Science (e.g. Computer Vision, Machine Learning, Theory), and the resulting algorithms have greatly helped in obtaining accurate and reliable solutions to many problems. These algorithms are extremely efficient and can find the globally (or strong locally) optimal solutions for an important class of models in polynomial time. Hence, they have led to a significant increase in the use of random field models in computer vision and information engineering in general. This tutorial is aimed at researchers who wish to use and understand these algorithms for solving new problems in computer vision and information engineering. No prior knowledge of probabilistic models or discrete optimization will be assumed. The tutorial will answer the following questions: (a) How to formalize and solve some known vision problems using MAP inference of a random field? (b) What are the different genres of MAP inference algorithms? (c) How do they work? (d) What are the recent developments and open questions in this field?"
http://videolectures.net/bmvc2012_sclaroff_communication/,"This talk will give an overview of some of the research in the Image and Video Computing Group at Boston University related to tracking, analysis, recognition and retrieval of images and video based on humans and their actions. First, efficient methods for inference of human pose will be presented. Linearly augmented tree models are proposed that enable efficient scale and rotation invariant matching. In another approach, articulated pose estimation with loopy graph models is made efficient via a branch-and-bound strategy for finding the globally optimal pose. Second, methods for learning human action models from Web images and video will be presented; the methods require no human intervention other than the action keywords to be used to form text queries to Web image and video search engines. A Multiple Instance Learning framework for exploiting properties of the scene, objects, and humans in video is also proposed. Third, work towards automatic recognition and retrieval of American Sign Language (ASL) in video databases will be presented. The goal is to enable users to search ASL video content simply by video-recording a query sign and relying on computer-based sign-recognition for lookup."
http://videolectures.net/bmvc2012_quattoni_structure_prediction/,"In the first part of the talk I will present recent work on learning latent variable models for content-based image retrieval. To learn a function that predicts the relevance of a database image to an image query all that we need is some form of feedback from users of the retrieval system. For example, we can obtain triplet constraints specifying that relative to some query Q, an image A should be ranked higher than an image B. When such feedback is available ranking SVMs can be used to induce the retrieval function. I will describe an extension of this framework where instead of learning a single relevance function we learn a mixture of relevance functions. Intuitively, given a query we first compute a distribution over ""coarse"" latent classes and then compute the relevance function for queries of that class. I will present a simple learning algorithm that induces both the latent classes and the parameters of each model. In the second part of the talk I will describe some of my current work on developing efficient learning algorithms for structure prediction with latent variables. These algorithms are based on using an algebraic representation that exploits directly the markovianity of the distribution."
http://videolectures.net/bmvc2012_davison_scene_perception/,"We have seen great advances in real-time 3D vision in recent years, enabled by algorithmic improvements, the continuing increase in commodity processing power and better camera technology. Research in Monocular SLAM (Simultaneous Localisation and Mapping), where a single agile camera moves through a mostly static scene, was for a long time focused on mapping only enough of a scene to enable robust real-time motion estimation of the camera itself. Attention is now turning however to gradually improving the quality of scene reconstruction which can be achieved in real-time. I will speak about how early work on feature-based SLAM is now being surpassed by methods which aim to map dense scene structure, and how this is leading towards ever-more general 3D scene modelling and understanding."
http://videolectures.net/bmvc2012_matas_visual_tracking/,"Visual tracking is an old area that has recently seen a surge in activity. The interest has been fueled by progress in related fields like detection, segmentation and optic flow as well as by application-driven demand and the increase in the available computing power. The published tracking methods differ in many aspects such as the speed, the complexity of the model of the tracked entity, the (geometric) transformations assumed, the mode of operation (casual and non-causal), the ability to adapt and learn, the robustness to occlusion and assumptions about the observer. I will review the dataset used in recent publications and show that the ""tracker space"" is still wide open with large areas to be explored. I will then present three trackers developed by me and my collaborators that operate at very different points in the speed-robustness-flexibility space that are close to the ""convex hull"" of published methods: the TLD tracker, the Flock-of-Trackers and the Zero-Shift-Point tracker. I will focus on a common aspect shared by the trackers: mechanisms for prediction and handling of tracking errors. Such mechanisms contribute to tracker robustness, which will be demonstrated live."
http://videolectures.net/bmvc2012_pfister_sign_language/,"We present a fully automatic arm and hand tracker that detects joint positions over continuous sign language video sequences of more than an hour in length. Our framework replicates the state-of-the-art long term tracker by Buehler et al. (IJCV 2011), but does not require the manual annotation and, after automatic initialisation, performs tracking in real-time. We cast the problem as a generic frame-by-frame random forest regressor without a strong spatial model. Our contributions are (i) a co-segmentation algorithm that automatically separates the signer from any signed TV broadcast using a generative layered model; (ii) a method of predicting joint positions given only the segmentation and a colour model using a random forest regressor; and (iii) demonstrating that the random forest can be trained from an existing semi-automatic, but computationally expensive, tracker. The method is applied to signing footage with changing background, challenging imaging conditions, and for different signers. We achieve superior joint localisation results to those obtained using the method of Buehler et al."
http://videolectures.net/bmvc2012_padoy_curvilinear_objects/,"The evaluation and automation of tasks involving the manipulation of deformable curvilinear objects, such as threads and cables, requires the real-time estimation of the 3D shapes of these objects from images. This estimation is however extremely challenging due to the small amount of available visual information, the inherent geometric ambiguities, and the large object deformations. We propose an approach for tracking deformable curvilinear objects using solely visual information from one or more calibrated cameras. The key idea is to formulate the shape estimation as a deformable 1D template tracking problem. The object is first textured with a pattern of different alternating colors. The tracking problem is then expressed as an energy minimization over a set of control points parameterizing a 3D NURBS modeling the object. Assuming the object’s in-extensibility, we propose a novel energy based on a texture-sensitive distance map. We demonstrate the benefits of this energy in synthetic and real experiments, using data illustrating the deformation and manipulation of a thread with a da Vinci robot. In particular, we show that the approach allows for deformable tracking in the absence of normal motion along the curve, a challenging practical situation that occurs when the thread is dragged by one extremity."
http://videolectures.net/bmvc2012_kazemi_pose_estimation/,"We present a fully automatic procedure for reconstructing the pose of a person in 3D from images taken from multiple views. We demonstrate a novel approach for learning more complex models using SVM-Rank, to reorder a set of high scoring configurations. The new model in many cases can resolve the problem of double counting of limbs which happens often in the pictorial structure based models. We address the problem of flipping ambiguity to find the correct correspondences of 2D predictions across all views. We obtain improvements for 2D prediction over the state of art methods on our dataset. We show that the results in many cases are good enough for a fully automatic 3D reconstruction with uncalibrated cameras."
http://videolectures.net/bmvc2012_mitzel_human_detection/,"In this paper we consider the problem of multi-person detection from the perspective of a head mounted stereo camera. As pedestrians close to the camera cannot be detected by classical full-body detectors due to strong occlusion, we propose a stereo depth-template based detection approach for close-range pedestrians. We perform a sliding window procedure, where we measure the similarity between a learned depth template and the depth image. To reduce the search space of the detector we slide the detector only over few selected regions of interest that are generated based on depth information. The region-of-interest selection allows us to further constrain the number of scales to be evaluated, significantly reducing the computational cost. We present experiments on stereo sequences recorded from a head-mounted camera setup in crowded shopping street scenarios and show that our proposed approach achieves superior performance on this very challenging data."
http://videolectures.net/bmvc2012_tang_occluded_people/,"We consider the problem of detection and tracking of multiple people in crowded street scenes. State-of-the-art methods perform well in scenes with relatively few people, but are severely challenged by scenes with many subjects that partially occlude each other. This limitation is due to the fact that current people detectors fail when persons are strongly occluded. We observe that typical occlusions are due to overlaps between people and propose a people detector tailored to various occlusion levels. Instead of treating partial occlusions as distractions, we leverage the fact that person/person occlusions result in very characteristic appearance patterns that can help to improve detection results. We demonstrate the performance of our occlusion-aware person detector on a new dataset of people with controlled but severe levels of occlusion and on two challenging publicly available benchmarks outperforming single person detectors in each case."
http://videolectures.net/bmvc2012_ladicky_deformation_field/,"Methods for human detection and localization typically use histograms of gradients (HOG) and work well for aligned data with low variance. For methods based on HOG despite the fact the higher resolution templates capture more details, their use does not lead to a better performance, because even a small variance in the data could cause the discriminative edges to fall into different neighbouring cells. To overcome these problems, Felzenszwalb et al. proposed a star-graph part based deformable model with a fixed number of rigid parts, which could capture these variations in the data leading to state-ofthe- art results. Motivated by this work, we propose a latent deformable template model with a locally affine deformation field, which allows for more general and more natural deformations of the template while not over-fitting the data; and we also provide a novel inference method for this kind of problem. This deformation model gives us a way to measure the distances between training samples, and we show how this can be used to cluster the problem into several modes, corresponding to different types of objects, viewpoints or poses. Our method leads to a significant improvement over the state-of-the-art with small computational overhead."
http://videolectures.net/bmvc2012_erbs_stixmentation/,"The detection of moving objects like vehicles, pedestrians or bicycles from a mobile platform is one of the most challenging and most important tasks for driver assistance and safety systems. For this purpose, we present a multi-class traffic scene segmentation approach based on the Dynamic Stixel World, an efficient super-pixel object representation. In this approach, each Stixel is assigned either to a quantized maneuver motion class like oncoming, or left-moving or to static background. The formulation integrates multiple 3D and motion features as well as spatio-temporal prior knowledge in a probabilistic conditional random field (CRF) framework. The real-time capable method is evaluated quantitatively in various challenging, cluttered urban traffic scenes. The experimental results yield highly accurate segmentation of urban traffic scenarios without the need for any manual parameter adjustments."
http://videolectures.net/bmvc2012_budvytis_video_segmentation/,We present a novel mixture of trees (MoT) graphical model for video segmentation. Each component in this mixture represents a tree structured temporal linkage between super-pixels from the first to the last frame of a video sequence. Our time-series model explicitly captures the uncertainty in temporal linkage between adjacent frames which improves segmentation accuracy. We provide a variational inference scheme for this model to estimate super-pixel labels and their confidences in nearly realtime. The efficacy of our approach is demonstrated via quantitative comparisons on the challenging SegTrack joint segmentation and tracking dataset.
http://videolectures.net/bmvc2012_vineet_mean_field/,"Recently, Krahenbuhl and Koltun proposed an efficient inference method for densely connected pairwise random fields using the mean-field approximation for a Conditional Random Field (CRF). However, they restrict their pairwise weights to take the form of a weighted combination of Gaussian kernels where each Gaussian component is allowed to take only zero mean, and can only be rescaled by a single value for each label pair. Further, their method is sensitive to initialization. In this paper, we propose methods to alleviate these issues. First, we propose a hierarchical mean-field approach where labelling from the coarser level is propagated to the finer level for better initialisation. Further, we use SIFT-flow based label transfer to provide a good initial condition at the coarsest level. Second, we allow our approach to take general Gaussian pairwise weights, where we learn the mean, the co-variance matrix, and the mixing co-efficient for every mixture component. We propose a variation of Expectation Maximization (EM) for piecewise learning of the parameters of the mixture model determined by the maximum likelihood function. Finally, we demonstrate the efficiency and accuracy offered by our method for object class segmentation problems on two challenging datasets: PascalVOC-10 segmentation and CamVid datasets. We show that we are able to achieve state of the art performance on the CamVid dataset, and an almost 3% improvement on the PascalVOC- 10 dataset compared to baseline graph-cut and mean-field methods, while also reducing the inference time by almost a factor of 3 compared to graph-cuts based methods."
http://videolectures.net/bmvc2012_taniai_distribution_matching/,"We propose an image segmentation method that divides an image into foreground and background regions when the approximate color distributions for these regions are given. Our approach was inspired by global consistency measures that directly evaluate the similarity between a given distribution and the distribution of the resulting segmentation, which were recently proposed in order to overcome the limitations of traditional pixelwise (local) consistency measures. The main feature of our proposal is that it uses two (foreground and background) input distributions, which increases the robustness compared to previous studies. To achieve this, we formulated a new mathematical model that describes the consistencies between the two input distributions and the segmentation, in which weighting parameters for the two distribution matching terms are set to be approximately proportional to the size of the foreground and background areas. We call this dual distribution matching (DDM). We also derived an optimization method that uses graph cuts. Experimental results that show the effectiveness of our method and comparisons between local and global consistency measures are presented."
http://videolectures.net/bmvc2012_sattler_image_retrieval/,"To reliably determine the camera pose of an image relative to a 3D point cloud of a scene, correspondences between 2D features and 3D points are needed. Recent work has demonstrated that directly matching the features against the points outperforms methods that take an intermediate image retrieval step in terms of the number of images that can be localized successfully. Yet, direct matching is inherently less scalable than retrievalbased approaches. In this paper, we therefore analyze the algorithmic factors that cause the performance gap and identify false positive votes as the main source of the gap. Based on a detailed experimental evaluation, we show that retrieval methods using a selective voting scheme are able to outperform state-of-the-art direct matching methods. We explore how both selective voting and correspondence computation can be accelerated by using a Hamming embedding of feature descriptors. Furthermore, we introduce a new dataset with challenging query images for the evaluation of image-based localization."
http://videolectures.net/bmvc2012_tighe_image_collections/,"In this work, we address the issue of geometric verification, with a focus on modeling large-scale landmark image collections gathered from the internet. In particular, we show that we can compute and learn descriptive statistics pertaining to the image collection by leveraging information that arises as a by-product of the matching and verification stages. Our approach is based on the intuition that validating numerous image pairs of the same geometric scene structures quickly reveals useful information about two aspects of the image collection: (a) the reliability of individual visual words and (b) the appearance of landmarks in the image collection. Both of these sources of information can then be used to drive any subsequent processing, thus allowing the system to bootstrap itself. While current techniques make use of dedicated training/preprocessing stages, our approach elegantly integrates into the standard geometric verification pipeline, by simply leveraging the information revealed during the verification stage. The main result of this work is that this unsupervised “learning-as-you-go” approach significantly improves performance; our experiments demonstrate significant improvements in efficiency and completeness over standard techniques."
http://videolectures.net/bmvc2012_shi_object_annotation/,"Most existing approaches to training object detectors rely on fully supervised learning, which requires the tedious manual annotation of object location in a training set. Recently there has been an increasing interest in developing weakly supervised approach to detector training where the object location is not manually annotated but automatically determined based on binary (weak) labels indicating if a training image contains the object. This is a challenging problem because each image can contain many candidate object locations which partially overlaps the object of interest. Existing approaches focus on how to best utilise the binary labels for object location annotation. In this paper we propose to solve this problem from a very different perspective by casting it as a transfer learning problem. Specifically, we formulate a novel transfer learning based on learning to rank, which effectively transfers a model for automatic annotation of object location from an auxiliary dataset to a target dataset with completely unrelated object categories. We show that our approach outperforms existing state-of-the-art weakly supervised approach to annotating objects in the challenging VOC dataset."
http://videolectures.net/bmvc2012_aytar_transfer_regularization/,"Exemplar SVMs (E-SVMs, Malisiewicz et al, ICCV 2011), where a SVM is trained with only a single positive sample, have found applications in the areas of object detection and Content-Based Image Retrieval (CBIR), amongst others. In this paper we introduce a method of part based transfer regularization that boosts the performance of E-SVMs, with a negligible additional cost. This Enhanced E-SVM (EE-SVM) improves the generalization ability of E-SVMs by softly forcing it to be constructed from existing classifier parts cropped from previously learned classifiers. In CBIR applications, where the aim is to retrieve instances of the same object class in a similar pose, the EE-SVM is able to tolerate increased levels of intra-class variation and deformation over E-SVM, and thereby increases recall. We make the following contributions: (a) introduce the EE-SVM objective function; (b) demonstrate the improvement in performance of EE-SVM over E-SVM for CBIR; and, (c) show that there is an equivalence between transfer regularization and feature augmentation for this problem and others, with the consequence that the new objective function can be optimized using standard libraries. EE-SVM is evaluated both quantitatively and qualitatively on the PASCAL VOC 2007 and ImageNet datasets for pose specific object retrieval. It achieves a significant performance improvement over E-SVMs, with greater suppression of negative detections and increased recall, whilst maintaining the same ease of training and testing."
http://videolectures.net/bmvc2012_fowlkes_object_detection/,"Datasets for training object recognition systems are steadily growing in size. This paper investigates the question of whether existing detectors will continue to improve as data grows, or if models are close to saturating due to limited model complexity and the Bayes risk associated with the feature spaces in which they operate. We focus on the popular paradigm of scanning-window templates defined on oriented gradient features, trained with discriminative classifiers. We investigate the performance of mixtures of templates as a function of the number of templates (complexity) and the amount of training data. We find that additional data does help, but only with correct regularization and treatment of noisy examples or “outliers” in the training data. Surprisingly, the performance of problem domain-agnostic mixture models appears to saturate quickly (10 templates and 100 positive training examples per template). However, compositional mixtures (implemented via composed parts) give much better performance because they share parameters among templates, and can synthesize new templates not encountered during training. This suggests there is still room to improve performance with linear classifiers and the existing feature space by improved representations and learning algorithms."
http://videolectures.net/bmvc2012_li_scene_understanding/,"Hierarchical methods have been widely explored for object recognition, which is a critical component of scene understanding. However, few existing works are able to model the contextual information (e.g., objects co-occurrence) explicitly within a single coherent framework for scene understanding. Towards this goal, in this paper we propose a novel three-level (superpixel level, object level and scene level) hierarchical model to address the scene categorization problem. Our proposed model is a coherent probabilistic graphical model that captures the object co-occurrence information for scene understanding with a probabilistic chain structure. The efficacy of the proposed model is demonstrated by conducting experiments on the LabelMe dataset."
http://videolectures.net/bmvc2012_alexiou_categorical_opponency/,"Recent progress has been made on sparse dictionaries for the Bag-of-Visual-Words (BOVW) approach to object recognition and scene categorization. In particular, jointly encoded words have been shown to greatly enhance retrieval and categorization performance by both improving dictionary sparsity, which impacts efficiency of retrieval, and improving the selectivity of categorization. In this paper, we suggest and evaluate different functions for the “soft-pairing” of words, whereby the likelihood of pairing is influenced by proximity and scale of putative word pairs. The methods are evaluated in both the Caltech-101 database and the Pascal VOC 2007 and 2011 databases. The results are compared against spatial pyramids using BOVW descriptions, standard BOVW approaches, and across different parameter values of pairing functions. We also compare dense and keypoint-based approaches in this context. One conclusion is that word pairing provides a means towards attaining the performance of much larger dictionary sizes without the computational effort of clustering. This lends it to situations where the dictionaries must be frequently relearned, or where image statistics frequently change."
http://videolectures.net/bmvc2012_sapienza_labelled_videos/,"Current state-of-the-art action classification methods extract feature representations from the entire video clip in which the action unfolds, however this representation may include irrelevant scene context and movements which are shared amongst multiple action classes. For example, a waving action may be performed whilst walking, however if the walking movement and scene context appear in other action classes, then they should not be included in a waving movement classifier. In this work, we propose an action classification framework in which more discriminative action subvolumes are learned in a weakly supervised setting, owing to the difficulty of manually labelling massive video datasets. The learned models are used to simultaneously classify video clips and to localise actions to a given space-time subvolume. Each subvolume is cast as a bag-offeatures (BoF) instance in a multiple-instance-learning framework, which in turn is used to learn its class membership. We demonstrate quantitatively that even with single fixedsized subvolumes, the classification performance of our proposed algorithm is superior to the state-of-the-art BoF baseline on the majority of performance measures, and shows promise for space-time action localisation on the most challenging video datasets."
http://videolectures.net/bmvc2012_zheng_transferable_dictionary/,"Discriminative appearance features are effective for recognizing actions in a fixed view, but generalize poorly to changes in viewpoint. We present a method for viewinvariant action recognition based on sparse representations using a transferable dictionary pair. A transferable dictionary pair consists of two dictionaries that correspond to the source and target views respectively. The two dictionaries are learned simultaneously from pairs of videos taken at different views and aim to encourage each video in the pair to have the same sparse representation. Thus, the transferable dictionary pair links features between the two views that are useful for action recognition. Both unsupervised and supervised algorithms are presented for learning transferable dictionary pairs. Using the sparse representation as features, a classifier built in the source view can be directly transferred to the target view. We extend our approach to transferring an action model learned from multiple source views to one target view. We demonstrate the effectiveness of our approach on the multi-view IXMAS data set. Our results compare favorably to the the state of the art."
http://videolectures.net/bmvc2012_fu_video_retrieval/,"In this work, we focus on developing features and approaches to represent and analyze videography styles in unconstrained videos. By unconstrained videos, we mean typical consumer videos with significant content complexity and diverse editing artifacts, mostly with long duration. Our approach constructs a videography dictionary, which is used to represent each video clip as a series of varying videography words. In addition to conventional features such as camera motion and foreground object motion, two novel features including motion correlation and scale information are introduced to characterize videography. Then, we show that unique videography signatures from different events can be automatically identified, using statistical analysis methods. For practical applications, we explore the use of videography analysis for content-based video retrieval and video summarization. We compare our approaches with other methods on a large unconstrained video dataset, and demonstrate that our approach benefits video analysis."
http://videolectures.net/bmvc2012_satkin_scene_understanding/,"In this paper, we propose a data-driven approach to leverage repositories of 3D models for scene understanding. Our ability to relate what we see in an image to a large collection of 3D models allows us to transfer information from these models, creating a rich understanding of the scene. We develop a framework for auto-calibrating a camera, rendering 3D models from the viewpoint an image was taken, and computing a similarity measure between each 3D model and an input image. We demonstrate this data-driven approach in the context of geometry estimation and show the ability to find the identities and poses of object in a scene. Additionally, we present a new dataset with annotated scene geometry. This data allows us to measure the performance of our algorithm in 3D, rather than in the image plane."
http://videolectures.net/bmvc2012_berg_face_verification/,"We propose a method of face verification that takes advantage of a reference set of faces, disjoint by identity from the test faces, labeled with identity and face part locations. The reference set is used in two ways. First, we use it to perform an “identity-preserving” alignment, warping the faces in a way that reduces differences due to pose and expression but preserves differences that indicate identity. Second, using the aligned faces, we learn a large set of identity classifiers, each trained on images of just two people. We call these “Tom-vs-Pete” classifiers to stress their binary nature. We assemble a collection of these classifiers able to discriminate among a wide variety of subjects and use their outputs as features in a same-or-different classifier on face pairs. We evaluate our method on the Labeled Faces in the Wild benchmark, achieving an accuracy of 93.10%, significantly improving on the published state of the art."
http://videolectures.net/bmvc2012_martins_conjugate_priors/,"This work presents a novel Bayesian formulation for aligning faces in unseen images. Our approach is closely related to Constrained Local Models (CLM) and Active Shape Models (ASM), where an ensemble of local feature detectors are constrained to lie within the subspace spanned by a Point Distribution Model (PDM). Fitting a model to an image typically involves two steps: a local search using a detector, obtaining response maps for each landmark (likelihood term) and a global optimization that finds the PDM parameters that jointly maximize all the detection responses. The global optimization can be seen as a Bayesian inference problem, where the posterior distribution of the PDM parameters (including pose) can be inferred in a maximum a posteriori (MAP) sense. Faces are nonrigid structures described by continuous dynamic transitions, so it is crucial to account for the underlying dynamics of the shape. We present a novel Bayesian global optimization strategy, where the prior is used to encode the dynamic transitions of the PDM parameters. Using recursive Bayesian estimation we model the prior distribution of the data as being Gaussian. The mean and covariance were assumed to be unknown and treated as random variables. This means that we estimate not only the mean and the covariance but also the probability distribution of the mean and the covariance (using conjugate priors). Extensive evaluations were performed on several standard datasets (IMM, BioID, XM2VTS and FGNET Talking Face) against state-ofthe- art methods while using the same local detectors. Finally, qualitative results taken from the challenging Labeled Faces in the Wild (LFW) dataset are also shown."
http://videolectures.net/bmvc2012_anderson_spanning_tree/,"We present a method for producing dense Active Appearance Models (AAMs), suitable for video-realistic synthesis. To this end we estimate a joint alignment of all training images using a set of pairwise registrations and ensure that these pairwise registrations are only calculated between similar images. This is achieved by defining a graph on the image set whose edge weights correspond to registration errors and computing a bounded diameter minimum spanning tree (BDMST). Dense optical flow is used to compute pairwise registration and we introduce a flow refinement method to align small scale texture. Once registration between training images has been established we propose a method to add vertices to the AAM in a way that minimises error between the observed flow fields and a flow field interpolated between the AAM mesh points. We demonstrate a significant improvement in model compactness using the proposed method and show it dealing with cases that are problematic for current state-of-the-art approaches."
http://videolectures.net/bmvc2012_besse_belief_propagation/,"PatchMatch is a simple, yet very powerful and successful method for optimizing continuous labelling problems. The algorithm has two main ingredients: the update of the solution space by sampling and the use of the spatial neighbourhood to propagate samples. We show how these ingredients are related to steps in a specific form of belief propagation in the continuous space, called Particle Belief Propagation (PBP). However, PBP has thus far been too slow to allow complex state spaces. We show that unifying the two approaches yields a new algorithm, PMBP, which is more accurate than PatchMatch and orders of magnitude faster than PBP. To illustrate the benefits of our PMBP method we have built a new stereo matching algorithm with unary terms which are borrowed from the recent PatchMatch Stereo work and novel realistic pairwise terms that provide smoothness. We have experimentally verified that our method is an improvement over state-of-the-art techniques at sub-pixel accuracy level."
http://videolectures.net/bmvc2012_alcantarilla_deformable_deconstruction/,"Deformable 3D reconstruction from 2D images requires prior knowledge on the scene structure. Template-free methods use generic prior knowledge such as piecewise smoothness but require multiple images with significant baseline. Template-based methods require only one image but handle only one object for which they need specific prior knowledge, namely a 3D template. We here propose a novel method that alleviates the strong assumptions of both the template-free and template-based methods: our method uses multiple templates to achieve deformable 3D reconstruction from only one image and for multiple objects. It uses object recognition to automatically discover what objects are visible in the input image and to select the appropriate templates for deformable 3D reconstruction. The object database is built offline. Crucially, this database does not only contain appearance descriptors as in existing object recognition frameworks, but also material properties to facilitate deformable 3D reconstruction. We show successful experimental results with objects made of various materials such as paper, cloth and plastic."
http://videolectures.net/bmvc2012_indelman_bundle_adjustment/,"Fast and reliable bundle adjustment is essential in many applications such as mobile vision, augmented reality, and robotics. Two recent ideas to reduce the associated computational cost are structure-less SFM (structure from motion) and incremental smoothing. The former formulates the cost function in terms of multi-view constraints instead of re-projection error, thereby eliminating the 3D structure from the optimization. The latter was developed in the SLAM (simultaneous localization and mapping) community and allows one to perform efficient incremental optimization, adaptively identifying the variables that need to be recomputed at each step. In this paper we combine these two key ideas into a computationally efficient bundle adjustment method, and additionally introduce the use of three-view constraints to remedy commonly encountered degenerate camera motions. We formulate the problem in terms of a factor graph, and incrementally update a directed junction tree which keeps track of the current best solution. Typically, only a small fraction of the camera poses are recalculated in each optimization step, leading to a significant computational gain. If desired, all or some of the observed 3D points can be reconstructed based on the optimized camera poses. To deal with degenerate motions, we use both two and three-view constraints between camera poses, which allows us to maintain a consistent scale during straight-line trajectories. We validate our approach using synthetic and real-imagery datasets and compare it to standard bundle adjustment, in terms of performance, robustness and computational cost."
http://videolectures.net/bmvc2012_bevilacqua_neighbor_embedding/,"This paper describes a single-image super-resolution (SR) algorithm based on nonnegative neighbor embedding. It belongs to the family of single-image example-based SR algorithms, since it uses a dictionary of low resolution (LR) and high resolution (HR) trained patch pairs to infer the unknown HR details. Each LR feature vector in the input image is expressed as the weighted combination of its K nearest neighbors in the dictionary; the corresponding HR feature vector is reconstructed under the assumption that the local LR embedding is preserved. Three key aspects are introduced in order to build a low-complexity competitive algorithm: (i) a compact but efficient representation of the patches (feature representation) (ii) an accurate estimation of the patches by their nearest neighbors (weight computation) (iii) a compact and already built (therefore external) dictionary, which allows a one-step upscaling. The neighbor embedding SR algorithm so designed is shown to give good visual results, comparable to other state-of-the-art methods, while presenting an appreciable reduction of the computational time."
http://videolectures.net/bmvc2012_hu_pose_subspace/,"Camera shake during exposure time often results in non-uniform blur across the entire image. Recent algorithms model the non-uniform blurry image as a linear combination of images observed by the camera at discretized poses, and focus on estimating the time fraction positioned at each pose. While these algorithms show promising results, they nevertheless entail heavy computational loads. In this work, we propose a novel single image deblurring algorithm to remove non-uniform blur. We estimate the local blur kernels at different image regions and obtain an initial guess of possible camera poses using backprojection. By restraining the possible camera poses in a low-dimensional subspace, we iteratively estimate the weight for each pose in the camera pose space. Experimental validations with the state-of-the-art methods demonstrate the efficiency and effectiveness of our algorithm for non-uniform deblurring."
