Video_Presentation,Abstracts
http://videolectures.net/eswc2014_staab_semantic_web/,"The Semantic Web changes the way we deal with data, because assumptions about the nature of the data that we deal with differ substantially from the ones in established database approaches. Semantic Web data is (i) provided by different people in an ad-hoc manner, (ii) distributed, (iii) semi-structured, (iv) (more or less) typed, (v) supposed to be used serendipitously. In fact, these are highly relevant assumptions and challenges, because they are frequently encountered in all kind of data-centric challenges – also in cases where Semantic Web standards are not in use. However, they are only partially accounted for in existing programming approaches for Semantic Web data including (i) semantic search, (ii) graph programming, and (iii) traditional database programming approaches.  The main hypothesis of this talk is that we have not yet developed the right kind of programming paradigms to deal with the proper nature of Semantic Web data, because none of the mentioned approaches fully considers its characteristics. Thus, I want to outline empirical investigations of Semantic Web data and recent developments towards Semantic Web programming that target the reduction of the impedance mismatch between data engineering and programming approaches."
http://videolectures.net/eswc2014_floridi_semantics_autonomy/,"The lecture is divided into four parts. In the first part, I offer a brief and simple introduction to four well-known senses in which different scientific fields speak of complexity, namely state complexity, Kolmogorov complexity, computational complexity, and programming complexity. I then suggest an intuitive way in which they can all be linked in a conceptual, unified view. Against this background, in the second part, I outline a new concept of complexity, which I shall call coordination complexity. This completes the unified view. I then argue, in the third part, that the semantic web helps us dealing with problems with increasingly high degree of coordination complexity, which require the mobilisation of whole systems to be tackled. In the last and concluding part, I highlight one of the consequences of the resolution of problems with high degree of coordination complexity: the predictability and manipulability of autonomous choices."
http://videolectures.net/eswc2014_tresp_machine_learning/,"Most successful applications of statistical machine learning focus on response learning or signal-reaction learning where an output is produced as a direct response to an input. An important feature is a quick response time, the basis for, e.g., real-time ad-placement on the Web, real-time address reading in postal automation, or a fast reaction to threats for a biological being. One might argue that knowledge about specific world entities and their relationships is necessary if the complexity of an agent's world increases, for example if an agent needs to function in a complex social community. As one is quite aware in the Semantic Web community, a natural representation of knowledge about entities and their relationships is a directed labeled graph where nodes represent entities and where a labeled link stands for a true fact. A number of successful graph-based knowledge representations, such as DBpedia, YAGO, or the Google Knowledge Graph, have recently been developed and are the basis of applications ranging from the support of search to the realization of question answering systems. Statistical machine learning can play an important role in knowledge graphs as well. By exploiting statistical relational patterns one can predict the likelihood of new facts, find entity clusters and determine if two entities refer to the same real world object. Furthermore, one can analyze new entities and map them to existing entities (recognition) and predict likely relations for the new entity. These learning tasks can elegantly be approached by first transforming the knowledge graph into a 3-way tensor where two of the modes represent the entities in the domain and the third mode represents the relation type. Generalization is achieved by tensor factorization using, e.g., the RESCAL approach. A particular feature of RESCAL is that it exhibits collective learning where information can propagate in the knowledge graph to support a learning task. In the presentation the RESCAL approach will be introduced and applications of RESCAL to different learning and decision tasks will be presented."
http://videolectures.net/eswc2014_presutti_panel/,"On March 12th, in occasion of the 25th birthday of the World Wide Web, Sir Tim Berners-Lee made an open call for a “Magna Carta” to protect Web users. He said: ""It's time for us to make a big communal decision. In front of us are two roads — which way are we going to go? Are we going to continue on the road and just allow the governments to do more and more and more control — more and more surveillance? Or are we going to set up a bunch of values? Are we going to set up something like a Magna Carta for the World Wide Web and say, actually, now it's so important, so much part of our lives, that it becomes on a level with human rights?"" We all know that guaranteeing privacy and security at the same time in a world without the WWW is a challenge, and that each country addresses it in a different way by means of local regulations as well as international agreements. Identifying the right tradeoff on the Web is probably even more complicated, as we deal with a virtual world, where geographical borders loose their importance and role.  The aim of this panel is to identify and discuss the most important issues that should be addressed in order to guarantee web users privacy, on one hand, without putting their security in danger, and web users security, on the other hand, without limiting their freedom by invading their private lives. Can semantics play any role in this trade-off?  We ask our panelists to summarize in three main statements their view on, and possibly their solution to, this problem, hence giving their advice to the definition of a Web users “Magna Carta”."
http://videolectures.net/eswc2014_alani_panel/,"Many projects, papers, and businesses, are centred around social media data, but how many comply fully with EU data protection directives and regulations?"" ""Do we even know how to translate and embed such laws into our tools and research practices?"" ""Can we take this challenge, and opportunity, to design and build semantic tools for lawful gathering and analysis of social data?"""
http://videolectures.net/eswc2014_gangemi_panel/,"""A little semantics goes a long way to both granting and harming a trade-off between privacy and security."" ""The trade-off between privacy and security is a suq-like issue, where the attempts to cleanly control it may create more harm than good."" ""As with the majority of human-accessible worlds, semantics is broken, but most humans know what is appropriate to do, given a certain context: probably the only reason why we haven't collapsed yet."""
http://videolectures.net/eswc2014_casanovas_panel/,"""Security and privacy (data protection) are two faces of the same coin."" ""Principles of fair information practices (FIPs) and ""privacy by design"" (PbD) deal also with metadata and the so-called ""identity meta-system"" layer of the Internet."" ""Implementation of security by design implies a democratic political turn towards a global digital neighborhood."""
http://videolectures.net/eswc2014_floridi_panel/,"""The EU’s current ethical focus on personal data protection is unbalanced."" ""It favours too much the protection of an individual’s privacy and right to be forgotten."" ""It entirely ignores the need to protect personal data when whole groups are in question."""
http://videolectures.net/eswc2014_gandon_panel/,"""Semantics are a double-edged weapon for security."" ""Deployment requires security on every floor."" ""Security is much more than a technical problem."""
http://videolectures.net/eswc2014_staab_panel/,"""Data protection and security is not an issue to be solved once and for all, but an enduring issue like traffic safety."" ""Data protection and security is and will remain inconvenient, even if the kind of inconvenience should (and will) shift."" ""Technology helps to encrypt, explain and suggest."""
http://videolectures.net/eswc2014_esther_vidal_panel/,"""The EU’s current ethical focus on personal data protection is unbalanced."" ""It favours too much the protection of an individual’s privacy and right to be forgotten."" ""It entirely ignores the need to protect personal data when whole groups are in question."""
http://videolectures.net/eswc2014_panel_data_protection/,"On March 12th, in occasion of the 25th birthday of the World Wide Web, Sir Tim Berners-Lee made an open call for a “Magna Carta” to protect Web users. He said: ""It's time for us to make a big communal decision. In front of us are two roads — which way are we going to go? Are we going to continue on the road and just allow the governments to do more and more and more control — more and more surveillance? Or are we going to set up a bunch of values? Are we going to set up something like a Magna Carta for the World Wide Web and say, actually, now it's so important, so much part of our lives, that it becomes on a level with human rights?"" We all know that guaranteeing privacy and security at the same time in a world without the WWW is a challenge, and that each country addresses it in a different way by means of local regulations as well as international agreements. Identifying the right tradeoff on the Web is probably even more complicated, as we deal with a virtual world, where geographical borders loose their importance and role. The aim of this panel is to identify and discuss the most important issues that should be addressed in order to guarantee web users privacy, on one hand, without putting their security in danger, and web users security, on the other hand, without limiting their freedom by invading their private lives. Can semantics play any role in this trade-off?  We ask our panelists to summarize in three main statements their view on, and possibly their solution to, this problem, hence giving their advice to the definition of a Web users “Magna Carta”."
http://videolectures.net/eswc2014_kuhn_linked_data/,"To make digital resources on the web verifiable, immutable, and permanent, we propose a technique to include cryptographic hash values in URIs. We call them trusty URIs and we show how they can be used for approaches like nanopublications to make not only specific resources but their entire reference trees verifiable. Digital artifacts can be identied not only on the byte level but on more abstract levels such as RDF graphs, which means that resources keep their hash values even when presented in a different format. Our approach sticks to the core principles of the web, namely openness and decentralized architecture, is fully compatible with existing standards and protocols, and can therefore be used right away. Evaluation of our reference implementations shows that these desired properties are indeed accomplished by our approach, and that it remains practical even for very large files."
http://videolectures.net/eswc2014_gong_entity_coreference/,"Entity coreference is important to Linked Data integration. User involvement is considered as a valuable source of human knowledge that helps identify coreferent entities. However, the quality of user involvement is not always satisfying, which significantly diminishes the coreference accuracy. In this paper, we propose a new approach called coCoref, which leverages distributed human computation and consensus partition for entity coreference. Consensus partition is used to aggregate all distributed user-judged coreference results and resolve their disagreements. To alleviate user involvement, ensemble learning is performed on the consensus partition to automatically identify coreferent entities that users have not judged. We integrate coCoref into an online Linked Data browsing system, so that users can participate in entity coreference with their daily Web activities. Our empirical evaluation shows that coCoref largely improves the accuracy of user-judged coreference results, and reduces user involvement by automatically identifying a large number of coreferent entities"
http://videolectures.net/eswc2014_ell_search_engine/,"In this paper we introduce Spartiqulation, a system that translates SPARQL queries into English text. Our aim is to allow casual end users of semantic applications with limited to no expertise in the SPARQL query language to interact with these applications in a more intuitive way. The verbalization approach exploits domain-independent template-based natural language generation techniques, as well as linguistic cues in labels and URIs"
http://videolectures.net/eswc2014_wang_linked_data/,"Due to the distributed nature of Linked Data, many resources are referred to by more than one URI. This phenomenon, known as co-reference, increases the probability of leaving out implicit semantically related results when querying Linked Data. The probability of co-reference increases further when considering distributed SPARQL queries over a larger set of distributed datasets. Addressing co-reference in Linked Data queries, on one hand, increases complexity of query processing. On the other hand, it requires changes in how statistics of datasets are taken into consideration. We investigate these two challenges of addressing co-reference in distributed SPARQL queries, and propose two methods to improve query efficiency: 1) a model named Virtual Graph, that trans- forms a query with co-reference into a normal query with pre-existing bindings; 2) an algorithm named, that intensively exploits parallelism, and dynamically optimises queries using runtime statistics. We deploy both methods in an distributed engine called LHD-d. To evaluate LHD-d, we investigate the distribution of co-reference in the real world, based on which we simulate an experimental RDF network. In this environment we demonstrate the advantages of LHD-d for distributed SPARQL queries in environments with co-reference"
http://videolectures.net/eswc2014_schaible_open_data/,"The choice of which vocabulary to reuse when modeling and publishing Linked Open Data (LOD) is far from trivial. There is no study that investigates the different strategies of reusing vocabularies for LOD modeling and publishing. In this paper, we present the results of a survey with 79 participants that examines the most preferred vocabulary reuse strategies of LOD modeling. The participants, LOD publishers and practitioners, were asked to assess different vocabulary reuse strategies and explain their ranking decision. We found significant differences between the modeling strategies that range from reusing popular vocabularies, minimizing the number of vocabularies, and staying within one do- main vocabulary. A very interesting insight is that the popularity in the meaning of how frequent a vocabulary is used in a data source is more important than how often individual classes and properties are used in the LOD cloud. Overall, the results of this survey help in better understanding the strategies how data engineers reuse vocabularies and may also be used to develop future vocabulary engineering tools."
http://videolectures.net/eswc2014_hasan_linked_data/,"Linked Data consumers may need explanations for debugging or understanding the reasoning behind producing the data. They may need the possibility to transform long explanations into more understandable short explanations. In this paper, we discuss an approach to explain reasoning over Linked Data. We introduce a vocabulary to describe explanation related metadata and we discuss how publishing these metadata as Linked Data enables explaining reasoning over Linked Data. Finally, we present an approach to summarize these explanations taking into account user specied explanation ltering criteria."
http://videolectures.net/eswc2014_rula_temporal_scopes/,"Information on the temporal interval of validity for facts described by RDF triples plays an important role in a large number of applications. Yet, most of the knowledge bases available on the Web of Data do not provide such information in an explicit manner. In this paper, we present a generic approach which addresses this drawback by inserting temporal information into knowledge bases. Our approach combines two types of information to associate RDF triples with time intervals. First, it relies on temporal information gathered from the document Web by an extension of the fact validation framework DeFacto. Second, it harnesses the time information contained in knowledge bases. This knowledge is combined within a three-step approach which comprises the steps matching, selection and merging. We evaluate our approach against a corpus of facts gathered from Yago2 by using DBpedia and Freebase as input and diferent parameter settings for the underlying algorithms. Our results suggest that we can detect temporal information for facts from DBpedia with an F-measure of up to 70%"
http://videolectures.net/eswc2014_paulheim_numerical_data/,"DBpedia is a central hub of Linked Open Data (LOD). Being based on crowd-sourced contents and heuristic extraction methods, it is not free of errors. In this paper, we study the application of unsupervised numerical outlier detection methods to DBpedia, using Interquantile Range (IQR), Kernel Density Estimation (KDE), and various dispersion estimators, combined with dierent semantic grouping methods. Our approach reaches 87% precision, and has lead to the identication of 11 systematic errors in the DBpedia extraction framework"
http://videolectures.net/eswc2014_fetahu_topic_profiles/,"The increasing adoption of Linked Data principles has led to an abundance of datasets on the Web. However, take-up and reuse is hindered by the lack of descriptive information about the nature of the data, such as their topic coverage, dynamics or evolution. To address this issue, we propose an approach for creating linked dataset proles. A prole consists of structured dataset metadata describing topics and their relevance. Proles are generated through the conguration of techniques for resource sampling from datasets, topic extraction from reference datasets and their ranking based on graphical models. To enable a good trade-o between scalability and accuracy of generated proles, appropriate parameters are determined experimentally. Our evaluation considers topic proles for all accessible datasets from the Linked Open Data cloud. The results show that our approach generates accurate proles even with comparably small sample sizes (10%) and outperforms established topic modelling approaches"
http://videolectures.net/eswc2014_cheng_entity_summaries/,"A primary challenge to Web data integration is coreference resolution, namely identifying entity descriptions from different data sources that refer to the same real-world entity. Increasingly, solutions to coreference resolution have humans in the loop. For instance, many active learning, crowdsourcing, and pay-as-you-go approaches so licit user feedback for verifying candidate coreferent entities computed by automatic methods. Whereas reducing the number of verification tasks is a major consideration for these approaches, very little attention has been paid to the efficiency of performing each single verification task. To address this issue, in this paper, instead of showing the entire descriptions of two entities for verification which are possibly lengthy, we propose to extract and present a compact summary of them, and expect that such length-limited comparative entity summaries can help human users verify more efficiently without significantly hurting the accuracy of the their verification. Our approach exploits the common and different features of two entities that best help indicate (non-)coreference, and also considers the diverse information on their identities. Experimental results show that verification is 2.7–2.9 times faster when using our comparative entity summaries, and its accuracy is not notably affected."
http://videolectures.net/eswc2014_wagner_web_of_data/,"For effectively searching the Web of data, ranking of results is a crucial. Top-k processing strategies have been proposed to allow an efficient processing of such ranked queries. Top-k strategies aim at computing k top-ranked results without complete result materialization. However, for many applications result computation time is much more important than result accuracy and completeness. Thus, there is a strong need for approximated ranked results. Unfortunately, previous work on approximate top-k processing is not well-suited for the Web of data. In this paper, we propose the first approximate top-k join framework for Web data and queries. Our approach is very lightweight necessary statistics are learned at runtime in a pay-as-you-go manner. We conducted extensive experiments on state-of-art SPARQL benchmarks. Our results are very promising: we could achieve up to 65% time savings, while maintaining a high precision/recall."
http://videolectures.net/eswc2014_kasten_graph_data/,"Existing algorithms for signing graph data typically do not cover the whole signing process. In addition, they lack distinctive features such as signing graph data at different levels of granularity, iterative signing of graph data, and signing multiple graphs. In this paper, we introduce a novel framework for signing arbitrary graph data provided, e g., as RDF(S), Named Graphs, or OWL. We conduct an extensive theoretical and empirical analysis of the runtime and space complexity of different framework configurations. The experiments are performed on synthetic and real-world graph data of different size and different number of blank nodes. We investigate security issues, present a trust model, and discuss practical considerations for using our signing framework"
http://videolectures.net/eswc2014_gottron_linked_data/,"In this paper we analyse the sensitivity of twelve prototypical Linked Data index models towards evolving data. Thus, we consider the reliability and accuracy of results obtained from an index in scenarios where the original data has changed after having been indexed. Our analysis is based on empirical observations over real world data covering a time span of more than one year. The quality of the index models is evaluated w.r.t. their ability to give reliable estimations of the distribution of the indexed data. To this end we use metrics such as perplexity, cross-entropy and Kullback-Leibler divergence. Our experiments show that all considered index models are affected by the evolution of data, but to different degrees and in different ways. We also make the interesting observation that index models based on schema information seem to be relatively stable for estimating densities even if the schema elements diverge a lot."
http://videolectures.net/eswc2014_saleem_endpoint_federation/,"Efficient federated query processing is of significant importance to tame the large amount of data available on the Web of Data. Previous works have focused on generating optimized query execution plans for fast result retrieval. However, devising source selection approaches beyond triple pattern-wise source selection has not received much attention. This work presents HiBISCuS, a novel hypergraph-based source selection approach to federated SPARQL querying. Our approach can be directly combined with existing SPARQL query federation engines to achieve the same recall while querying fewer data sources. We extend three well-known SPARQL query federation engines with HiBISCus and compare our extensions with the original approaches on FedBench. Our evaluation shows that HiBISCuS can efficiently reduce the total number of sources selected without losing recall. Moreover, our approach significantly reduces the execution time of the selected engines on most of the benchmark queries"
http://videolectures.net/eswc2014_lantzaki_blank_nodes/,"Generators for synthetic RDF datasets are very important for testing and benchmarking various semantic data management tasks (e.g. querying, storage, update, compare, integrate). How ever, the current generators do not support sufficiently (or totally ignore) blank node connectivity issues. Blank nodes are used for various purposes (e.g. for describing complex attributes), and a significant percentage of resources is currently represented with blank nodes. Moreover, several semantic data management tasks, like isomorphism checking (useful for checking equivalence), and blank node matching (useful in comparison, versioning, synchronization, and in semantic similarity functions), not only have to deal with blank nodes, but their complexity and optimality depends on the connectivity of blank nodes. To enable the comparative evaluation of the various techniques for carrying out these tasks, in this paper we present the design and implementation of a generator, called BGen, which allows building datasets containing blank nodes with the desired complexity, controllable through various features (morphology, size, diameter, density and clustering coefficient). Finally, the paper reports experimental results concerning the efficiency of the generator, as well as results from using the generated datasets, that demonstrate the valueof the generator"
http://videolectures.net/eswc2014_maccioni_keyword_search/,"Non expert users need support to access linked data available on the Web. To this aim, keyword-based search is considered an essential feature of database systems. The distributed nature of the Semantic Web demands query processing techniques to evolve towards a scenario where data is scattered on distributed data stores. Existing approaches to keyword search cannot guarantee scalability in a distributed environment, because, at runtime, they are unaware of the location of the relevant data to the query and thus, they cannot optimize join tasks. In this paper, we illustrate a novel distributed approach to keyword search over RDF data that exploits the MapReduce paradigm by switching the problem from graph-parallel to data-parallel processing. Moreover, our frame-work is able to consider ranking during the building phase to return directly the best (top-k) answers in the first (k) generated results, reducing greatly the overall computational load and complexity. Finally, a comprehensive evaluation demonstrates that our approach exhibits very good efficiency guaranteeing high level of accuracy, especially with respect to state-of-the-art competitors."
http://videolectures.net/eswc2014_gao_eviction_approach/,"Processing streams rather than static les of Linked Data has gained increasing importance in the web of data. When processing data-streams system builders are faced with the conundrum of guaranteeing a constant maximum response time with limited resources and, possibly, no prior information on the data arrival frequency. One approach to address this issue is to delete data from a cache during processing { a process we call eviction. The goal of this paper is to show that data-driven eviction outperforms today's dominant data-agnostic approaches such as rst-in-rst-out or random deletion. Specically, we rst introduce a method called Clock that evicts data from a join cache based on the likelihood estimate of contributing to a join in the future. Second, using the well-established SR-Bench bench-mark as well as a data set from the IPTV domain, we show that Clock out-performs data-agnostic approaches indicating its usefulness for resource-limited linked data stream processing."
http://videolectures.net/eswc2014_teymourian_event_streams/,"Background knowledge about the application domain can be used in event processing in order to improve processing quality. The idea of semantic enrichment is to incorporate background knowledge into events, thereby generating enriched events which, in the next processing step, can be better understood by event processing engines. In this paper, we present an efficient technique for event stream enrichment by planning multi-step event enrichment and processing. Our optimization goal is to minimize event enrichment costs while meeting application-specific service expectations. The event enrichment is optimized to avoid unnecessary event stream enrichment without missing any complex events of interest. Our experimental results shows that by using this approach it is possible to reduce the knowledge acquisition costs."
http://videolectures.net/eswc2014_costabello_linked_data/,"We present PRISSMA, a context-aware presentation layer for Linked Data. PRISSMA extends the Fresnel vocabulary with the notion of mobile context. Besides, it includes an algorithm that determines whether the sensed context is compatible with some context declarations. The algorithm nds optimal error-tolerant subgraph isomorphisms between RDF graphs using the notion of graph edit distance and is sublinear in the number of context declarations in the system"
http://videolectures.net/eswc2014_garcia_stream_compression/,"In many applications (like social or sensor networks) the information generated can be represented as a continuous stream of RDF items, where each item describes an application event (social network post, sensor measurement, etc). In this paper we focus on compressing RDF streams. In particular, we propose an approach for lossless RDF stream compression, named RDSZ (RDF Differential Stream compressor based on Zlib). This approach takes advantage of the structural similarities among items in a stream by combining a differential item encoding mechanism with the general purpose stream compressor Zlib. Empirical evaluation using several RDF stream datasets shows that this combination produces gains in compression ratios with respect to using Zlib alone"
http://videolectures.net/eswc2014_pedrinaci_service_trading/,"Real-world services ranging from cloud solutions to consulting currently dominate economic activity. Yet, despite the increasing number of service marketplaces online, service trading on the Web remains highly restricted. Services are at best traded within closed silos that oer mainly manual search and comparison capabilities through a Web storefront. Thus, it is seldom possible to automate the customisation, bundling, and trading of services, which would foster a more efficient and effective service sector. In this paper we present Linked USDL, a comprehensive vocabulary for capturing and sharing rich service descriptions, which aims to support the trading of services over the Web in an open, scalable, and highly automated manner. The vocabulary adopts and exploits Linked Data as a means to efficiently support communication over the Web, to promote and simplify its adoption by reusing vocabularies and datasets, and to enable the opportunistic engagement of multiple cross domain providers."
http://videolectures.net/eswc2014_peters_rule_based/,"Using semantic technologies the materialization of implicit given facts that can be derived from a dataset is an important task performed by a reasoner. With respect to the answering time for queries and the growing amount of available data, scaleable solutions that are able to process large datasets are needed. In previous work we described a rule-based reasoner implementation that uses massively parallel hardware to derive new facts based on a given set of rules. This implementation was limited by the size of processable input data as well as on the number of used parallel hardware devices. In this paper we introduce further concepts for a workload partitioning and distribution to overcome this limitations. Based on the introduced concepts, additional levels of parallelization can be proposed that benefit from the use of multiple parallel devices. Furthermore, we introduce a concept to reduce the amount of invalid triple derivations like duplicates. We evaluate our concepts by applying different rulesets to the real-world DBPedia dataset as well as to the synthetic Lehigh University benchmark ontology (LUBM) with up to 1.1 billion triples. The evaluation shows that our implementation scales in a linear way and outperforms current state of the art reasoner with respect to the throughput achieved on a single computing node"
http://videolectures.net/eswc2014_dutta_knowledge_sources/,"Open Information Extraction (OIE) systems like Nell and ReVerb have achieved impressive results by harvesting massive amounts of machine-readable knowledge with minimal supervision. However, the knowledge bases they produce still lack a clean, explicit semantic data model. This, on the other hand, could be provided by full- edged semantic networks like DBpedia or Yago, which, in turn, could benefit from the additional coverage provided by Web-scale IE. In this paper, we bring these two strains of research together, and present a method to align terms from Nell with instances in DBpedia. Our approach is unsupervised in nature and relies on two key components. First, we automatically acquire probabilistic type information for Nell terms given a set of matching hypotheses. Second, we view the mapping task as the statistical inference problem of finding the most likely coherent mapping i.e., the maximum a posteriori (MAP) mapping based on the outcome of the first component used as soft constraint. These two steps are highly intertwined: accordingly, we propose an approach that iteratively refines type acquisition based on the output of the mapping generator, and vice versa. Experimental results on gold-standard data indicate that our approach outperforms a strong baseline, and is able to produce ever-improving mappings consistently across iterations"
http://videolectures.net/eswc2014_cure_water_fowl/,"In this paper we present WaterFowl, a novel approach for the storage of RDF triples that addresses scalability issues through compression. The architecture of our prototype, largely based on the use of succinct data structures, enables the representation of triples in a self-indexed, compact manner without requiring decompression at query answering time. Moreover, it is adapted to efficiently support RDF and RDFS entailment regimes thanks to an optimized encoding of ontology concepts and properties that does not require a complete inference materialization or query reformulation. This approach implies to make a distinction between the terminological and the assertional components of the knowledge base early in the process of data preparation, i:e: preprocessing the data before storing it in our structures. The paper describes our system's architecture and presents some preliminary results obtained from evaluations on dierent datasets."
http://videolectures.net/eswc2014_stoilos_data_access/,"n previous work it has been shown how an OWL 2 DL ontology O can be `repaired' for an OWL 2 RL system ans that is, how we can compute a set of axioms R that is independent from the data and such that ans that is generally incomplete for O becomes complete for all SPARQL queries when used with O [ R. However, the initial implementation and experiments were very preliminary and hence it is currently unclear whether the approach can be applied to large and complex ontologies. Moreover, the approach so far can only support instance queries. In the current paper we thoroughly investigate repairing as an approach to scalable (and complete) ontology-based data access. First, we present several non-trivial optimisations to the rst prototype. Second, we show how (arbitrary) conjunctive queries can be supported by integrating well-known query rewriting techniques with OWL 2 RL systems via repairing. Third, we perform an extensive experimental evaluation obtaining encouraging results. In more detail, our results show that we can compute repairs even for very large real-world ontologies in a reasonable amount of time, that the performance overhead introduced by repairing is negligible in small to medium sized ontologies and noticeable but manageable in large and complex one, and that the hybrid reasoning approach can very efficiently compute the correct answers for real-world challenging scenarios"
