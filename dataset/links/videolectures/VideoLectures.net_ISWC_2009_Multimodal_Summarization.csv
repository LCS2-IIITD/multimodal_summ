Video_Presentation,Abstracts
http://videolectures.net/iswc09_hayes_blogic/,"Putting logic on the Web has seemed like an intellectual one-way street: the logic was all worked out a century ago or more, the technology is 20 years old, and we are simply dealing with the dirty practical business of putting it into XML and getting it onto the Web. But there needs to be some intellectual traffic in the other direction. When logic meets the Web we have to re-think several of the basic assumptions of logic itself, to the point that it should be seen as a new subject, with a new name: blogic. This talk surveys several foundational issues in blogic that either never arose previously in logic, or have to now be reconsidered, focussing particularly on issues arising from linked data and the need for an 'intimatelyRelatedButMaybeNotActuallySameAs' relation."
http://videolectures.net/iswc09_mitchell_ptsw/,"A key question to the future of the semantic web is ""how will we acquire structured information to populate the semantic web on a vast scale?"" One approach is to enter this information manually. A second approach is to take advantage of the great deal of structured information already present in various databases, and to develop common ontologies, publishing standards, and reward systems to make this data widely accessible. We consider here a third approach: developing software that automatically extracts structured information from unstructured text present on the web. This talk will survey attempts to extract structured knowledge from unstructured text, and will focus on an approach with three characteristics that we hypothesize make it viable. First, in contrast to the very difficult problem of reading information from a single document, we consider the much easier problem of reading hundreds of millions of documents simultaneously, so that our system can extract facts that are stated many times by combining evidence from many documents. Second, our system begins with a given ontology that defines the types of information to be extracted, enabling it to focus its effort and to ignore most of the text which is irrelevant to the target ontology. Third, the system uses a new class of semi-supervised learning algorithms to learn how to extract information from web pages -- algorithms designed to achieve greater accuracy when given more complex ontologies. Our experiments show that this approach can produce knowledge bases containing tens of thousands of facts to populate given ontologies with approximately 90% accuracy, starting with only a handful of labeled training examples and 200 million unlabeled web pages."
http://videolectures.net/iswc09_spivack_ppap/,"The next generation of Web search is coming sooner than expected. In fact, we are already seeing several shifts in the way people search, and the way major search engines provide functionality to consumers. Whereas Web 1.0 (1989-99) was defined by hierarchical rankings, and Web 2.0 (1999-2009) by social search, as we begin to realize the Semantic Web, the new paradigm of search will shift from the past to the present, and from the social to the personal. Relevancy will not just be defined by keywords and graph algorithms, but by semantic precision. Why should searches return the same results for everyone? When two different people search for the same information, they may want to get very different kinds of results. Someone who is a novice in a given field may want beginner-level information to rank higher in the results than someone who is an expert. Other use cases may emphasize things that are novel over things that have been seen before, or that have happened in the past â€” in these instances, the more timely something is, the more relevant it might be as well. Two themes -- present and personal -- will come to define great search experiences. And although timeliness and relevance are familiar (if nascent) concepts in the context of search today, this talk will focus in particular on exploring some of the new realities and long-term consequences of the decade to come."
