# multimodal_summ

#### Dataset: 

>Our dataset is arranged in the following tree directory.

Each folder contains a csv file pertatining to the relevant conference title. Every csv contains a Video/Paper/Page link along with the abstract which we use as a reference summary. For audio, Video features and Transcript generation, We have included our preprocessing codes. Audio features are represednted as MFCC vector, Video as ResNet and Transcripts are generated by Mozilla DeepSpeech. We also make use of OCR in video frames.

```
.
├── links
│   ├── acl_anthology
│   │   ├── 1
│   │   │   ├── acl
│   │   │   │   ├── ACL_2017.csv
│   │   │   │   ├── [...]
│   │   │   ├── emnlp
│   │   │   │   ├── EMNLP_2017.csv
│   │   │   │   └── [...]
│   │   │   ├── naacl
│   │   │   │   ├── NAACL_2018.csv
│   │   │   │   └── [...]
│   │   │   ├── tacl
│   │   │   │   ├── TACL_2013.csv
│   │   │   │   ├── [...]
│   │   │   └── ws
│   │   │       └── WS_2018.csv
│   │   └── 2
│   │       ├── emnlp
│   │       │   ├── EMNLP_2015.csv
│   │       │   └── EMNLP_2016.csv
│   │       └── ijcnlp
│   │           └── IJCNLP_2015.csv
│   ├── cvf
│   │   ├── CVF_CVPR_2016_Multimodal_Summarization.csv
│   │   ├── [...]
│   ├── icml_nips
│   │   ├── ICML_2017_Multimodal_Summarization.csv
│   │   ├── [...]
│   └── videolectures
│       ├── VideoLectures.net_BMVC_2009_Multimodal_Summarization.csv
│       ├── [...]

```

#### src

It contains codes for feature generation and the transformer model.

Audio features are generated by extracting the MFCC vectors (https://python-speech-features.readthedocs.io/en/latest/)

```
python3 src/runner_new.py
```

For video features we follow the steps provided by https://github.com/antoine77340/video_feature_extractor

```
python src/extract2.py --csv=input_path_video.csv --type=2d --batch_size=16 --num_decoding_thread=4
```

Transcripts are generated via https://github.com/mozilla/DeepSpeech

Model
```
python3 model_lm.py
```
